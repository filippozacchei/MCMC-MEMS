{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indentification of Adjusting Coefficients "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a gradient discent algorithm for identifing the parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import os \n",
    "import timeit\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "# Local module imports\n",
    "sys.path.append('../../src/SurrogateModeling')\n",
    "sys.path.append('../../src/InverseProblems')\n",
    "sys.path.append('../../src/utils')\n",
    "from utils import * \n",
    "\n",
    "# Surrogate Model Configurations\n",
    "CONFIGURATION_I = './config_I.json'\n",
    "data_processor_I = preprocessing(CONFIGURATION_I)\n",
    "\n",
    "# Extract test data \n",
    "X_train, y_train = data_processor_I.X_train, data_processor_I.y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stiffness(w, th , E , l1 , l2 , oe): \n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    w - Beam width\n",
    "    th - Beam thickness \n",
    "    E - Young's modulus of the beam material\n",
    "    l1 - Length of the longer beams\n",
    "    l2 - Length of the shorter beams\n",
    "    oe - Overetch affecting the effective width of the beam\n",
    "\n",
    "     Output:\n",
    "     kTotal - Total stiffness of the folded beam structure\n",
    "    \"\"\"\n",
    "    effectiveWidth = w - 2*oe\n",
    "    J = (1/12) * th * effectiveWidth**3\n",
    "    #Stiffness of individual beams based on their length\n",
    "    k1 = 12 * E * J / (l1**3 * 4)\n",
    "    k2 = 12 * E * J / (l2**3 * 2)\n",
    "    kTotal = 2 / (1/k1 + 1/k2)\n",
    "    \n",
    "    return kTotal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fringing_coeff( G, W ,L):\n",
    "    return (1+ G/np.pi/W + G/np.pi/W*np.log(2*np.pi*W/G))*(1+ G/np.pi/L + G/np.pi/L*np.log(2*np.pi*L/G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coarse_model(params, w0, w1, w2):\n",
    "    \"\"\"\n",
    "    - Inputs:\n",
    "    - parameters[0] : Overetch \n",
    "    - parameters[1] : Offset\n",
    "    - parameters[2] : Thickness\n",
    "\n",
    "    - Output:\n",
    "    - C : An array containing the computed difference of capacitance \n",
    "    \"\"\"\n",
    "    # Time parameters\n",
    "    t0 = 0                    # Initial time.\n",
    "    tf = 0.0015-1e-5          # Final time.\n",
    "    dt =1e-5                  # Time step size.\n",
    "\n",
    "    # Stiffness parameters\n",
    "    l1 = 221.4*1e-6              # Lenght of the longer beam \n",
    "    l2 = 110*1e-6            # Length of the shorted beam\n",
    "    E  = 160*1e9               # Young Modulus \n",
    "    w  = 2.8*1e-6              # Width \n",
    "\n",
    "    # Force parameters \n",
    "    phi = lambda t: 0.9*(1-np.cos(2*np.pi*2500*t)) if t < 2/2500 else 0 # Voltage in the right electrodes\n",
    "    s   = 101*1e-6             # param for the surface \n",
    "    dp  =1.2*1e-6                # Distance from the plates with Overetch and Offset = 0 \n",
    "\n",
    "    # Mass parameters\n",
    "    rho = 2320                 # Density of the mass.\n",
    "    A   = 84*1e-9              # Area of the component\n",
    "\n",
    "    # Damping parameters \n",
    "    alpha = 31440            # Damping coefficient alpha. 31400\n",
    "    beta  =  0               # Damping coeff beta \n",
    "\n",
    "    # Input Parameters \n",
    "    oe = params[0]*1e-6\n",
    "    of = params[1]*1e-6\n",
    "    th = params[2]*1e-6 # ricorda di rimettere 6\n",
    "   \n",
    "    eps0 = 8.854*1e-12       # Dielectric permittivity constant\n",
    "    eps1 = 1.000             # Relative dielectric permittivity of air.\n",
    "\n",
    "    # Compute the distance between the faces of electrodes and the sensor\n",
    "    dl = dp+2*oe+of\n",
    "    dr = dp+2*oe-of\n",
    "    # Compute the surface of the electrode \n",
    "    S = th * (s - 2*oe) * 10  # multiply by ten since we have 10 condensators\n",
    "    \n",
    "    # Initial conditions\n",
    "    u0 = 0  # Initial displacement\n",
    "    v0 = 0  # Initial velocity\n",
    "    N = int((tf - t0) / dt)\n",
    "\n",
    "    # Initialization\n",
    "    u = np.zeros((N+1))  # displacement\n",
    "    v = np.zeros((N+1))  # velocity\n",
    "    C = np.zeros((N+1))  # capacitance\n",
    "    u[0] = u0\n",
    "    v[0] = v0\n",
    "    C[0] = eps1*eps0*S*(1/(dr)*fringing_coeff( dr, s-2*oe ,th) - 1/(dl)*fringing_coeff( dl, s-2*oe ,th))\n",
    "\n",
    "    # Compute the stiffness\n",
    "    k = compute_stiffness(w, th , E , l1 , l2 , oe) \n",
    "   \n",
    "    # Compute mass\n",
    "    m = rho * A * th\n",
    "    # Compute Damping \n",
    "    damp = alpha* m + beta* k\n",
    "\n",
    "    # Precompute phi values to avoid redundant computation\n",
    "    k1 =  0.5 * eps0 * eps1 * S\n",
    "    k2 = eps1*eps0*S\n",
    "    F_values = np.array([phi(n * dt) for n in range(N)])**2 * k1\n",
    "\n",
    "    # Time-stepping loop using Forward Euler scheme\n",
    "    for n in range(N) :\n",
    "        u_n = u[n]\n",
    "        v_n = v[n]\n",
    "        u_new = u_n + dt * v_n\n",
    "        # Compute the value of the input voltage at time t = n*dt \n",
    "        F = F_values[n]/((dr-u_n)**2)\n",
    "        v[n+1] = v_n + dt * ( F - damp*v_n - k*u_n )/m\n",
    "        # Compute the difference of capacitance\n",
    "        C[n+1] = k2*(1/(dr-u_new)*fringing_coeff( dr-u_new, s-2*oe ,th) - 1/(dl+u_new)*fringing_coeff( dl+u_new, s-2*oe ,th))\n",
    "        u[n+1] = u_new\n",
    "    \n",
    "    # Adjusting phase \n",
    "    C = C*1.02*1e15 + w0*params[0] + w1*params[1] + w2*params[2]\n",
    "\n",
    "    return C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize the parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 283.4862060546875, Learning Rate: 0.0001\n",
      "Iteration 1, Loss: 191.63125610351562, Learning Rate: 4.0824829046386304e-05\n",
      "Iteration 2, Loss: 164.28277587890625, Learning Rate: 3.0151134457776364e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m w2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m150\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Run gradient descent\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m w0_optimized, w1_optimized, w2_optimized \u001b[38;5;241m=\u001b[39m gradient_descent(X_train, y_train, w0, w1, w2)\n",
      "Cell \u001b[0;32mIn[17], line 16\u001b[0m, in \u001b[0;36mgradient_descent\u001b[0;34m(X_train, y_train, w0, w1, w2, learning_rate, num_iterations, batch_size)\u001b[0m\n\u001b[1;32m     13\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y_train[batch_indices]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Compute predictions\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([coarse_model(X_batch[j, :], w0, w1, w2) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_batch))])\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(y_pred, y_batch)\n",
      "Cell \u001b[0;32mIn[17], line 16\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     13\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y_train[batch_indices]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Compute predictions\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([coarse_model(X_batch[j, :], w0, w1, w2) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_batch))])\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(y_pred, y_batch)\n",
      "Cell \u001b[0;32mIn[5], line 82\u001b[0m, in \u001b[0;36mcoarse_model\u001b[0;34m(params, w0, w1, w2)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Compute the value of the input voltage at time t = n*dt \u001b[39;00m\n\u001b[1;32m     81\u001b[0m F \u001b[38;5;241m=\u001b[39m F_values[n]\u001b[38;5;241m/\u001b[39m((dr\u001b[38;5;241m-\u001b[39mu_n)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 82\u001b[0m v[n\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m v_n \u001b[38;5;241m+\u001b[39m dt \u001b[38;5;241m*\u001b[39m ( F \u001b[38;5;241m-\u001b[39m damp\u001b[38;5;241m*\u001b[39mv_n \u001b[38;5;241m-\u001b[39m k\u001b[38;5;241m*\u001b[39mu_n )\u001b[38;5;241m/\u001b[39mm\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Compute the difference of capacitance\u001b[39;00m\n\u001b[1;32m     84\u001b[0m C[n\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m k2\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m(dr\u001b[38;5;241m-\u001b[39mu_new)\u001b[38;5;241m*\u001b[39mfringing_coeff( dr\u001b[38;5;241m-\u001b[39mu_new, s\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39moe ,th) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m(dl\u001b[38;5;241m+\u001b[39mu_new)\u001b[38;5;241m*\u001b[39mfringing_coeff( dl\u001b[38;5;241m+\u001b[39mu_new, s\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39moe ,th))\n",
      "File \u001b[0;32m~/miniconda3/envs/bima/lib/python3.11/site-packages/jax/_src/numpy/array_methods.py:259\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_defer_to_unrecognized_arg\u001b[39m(opchar, binary_op, swap\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    258\u001b[0m   \u001b[38;5;66;03m# Ensure that other array types have the chance to override arithmetic.\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m   \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeferring_binary_op\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(other, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__jax_array__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    261\u001b[0m       other \u001b[38;5;241m=\u001b[39m other\u001b[38;5;241m.\u001b[39m__jax_array__()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def loss_function(y_pred, y_true):\n",
    "    # Mean squared error loss\n",
    "    return np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "def gradient_descent(X_train, y_train, w0, w1, w2, learning_rate=0.0001, num_iterations=10000, batch_size=64):\n",
    "    num_samples = len(X_train)\n",
    "    for _ in range(num_iterations):\n",
    "        # Sample random indices for mini-batch\n",
    "        batch_indices = np.random.choice(num_samples, size=batch_size, replace=False)\n",
    "        X_batch = X_train[batch_indices]\n",
    "        y_batch = y_train[batch_indices]\n",
    "\n",
    "        # Compute predictions\n",
    "        y_pred = np.array([coarse_model(X_batch[j, :], w0, w1, w2) for j in range(len(X_batch))])\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_function(y_pred, y_batch)\n",
    "        \n",
    "        # Compute gradients\n",
    "        grad_w0 = np.mean(2 * (y_pred - y_batch) * X_batch[:, 0, np.newaxis], axis=0)\n",
    "        grad_w1 = np.mean(2 * (y_pred - y_batch) * X_batch[:, 1, np.newaxis], axis=0)\n",
    "        grad_w2 = np.mean(2 * (y_pred - y_batch) * X_batch[:, 2, np.newaxis], axis=0)\n",
    "        \n",
    "        # Update parameters with adaptive learning rate\n",
    "        current_learning_rate = np.max( (learning_rate / np.sqrt( 5*_ + 1), 1e-7) ) \n",
    "        w0 -= current_learning_rate * grad_w0\n",
    "        w1 -= current_learning_rate * grad_w1\n",
    "        w2 -= current_learning_rate * grad_w2\n",
    "        \n",
    "        # Print loss and learning rate for monitoring\n",
    "        print(\"Iteration {}, Loss: {}, Learning Rate: {}\".format(_, loss, current_learning_rate))\n",
    "        \n",
    "    return w0, w1, w2\n",
    "\n",
    "# Assuming X_train and y_train are already defined\n",
    "# Initialize parameters\n",
    "w0 = np.random.rand(150)\n",
    "w1 = np.random.rand(150)\n",
    "w2 = np.random.rand(150)\n",
    "\n",
    "# Run gradient descent\n",
    "w0_optimized, w1_optimized, w2_optimized = gradient_descent(X_train, y_train, w0, w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w0_optimized' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 60\u001b[0m\n\u001b[1;32m     57\u001b[0m w2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m150\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Run gradient descent\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m w0_optimized, w1_optimized, w2_optimized \u001b[38;5;241m=\u001b[39m gradient_descent(X_train, y_train, w0_optimized, w1_optimized, w2_optimized)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'w0_optimized' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def loss_function(y_pred, y_true):\n",
    "    # Mean squared error loss\n",
    "    return np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "def numerical_gradient(X_batch, y_batch, y_pred, w0, w1, w2, epsilon=1e-5):\n",
    "    # Compute numerical gradients using finite differences\n",
    "    gradients = []\n",
    "    for idx in range(len([w0, w1, w2])):\n",
    "        param_gradients = []\n",
    "        for i in range(len(w0)):\n",
    "            # Perturb parameter value\n",
    "            param_perturbed = np.copy([w0,w1,w2])\n",
    "            param_perturbed[idx][i] += epsilon\n",
    "\n",
    "            # Compute loss with perturbed parameter\n",
    "            y_pred_perturbed = np.array([coarse_model(X_batch[j, :], *param_perturbed) for j in range(len(X_batch))])\n",
    "            loss_perturbed = loss_function(y_pred_perturbed, y_batch)\n",
    "\n",
    "            # Compute gradient using finite differences\n",
    "            gradient = (loss_perturbed - loss_function(y_batch, y_pred)) / epsilon\n",
    "            param_gradients.append(gradient)\n",
    "        gradients.append(param_gradients)\n",
    "    return gradients\n",
    "\n",
    "def gradient_descent(X_train, y_train, w0, w1, w2, learning_rate=0.00001, num_iterations=10, batch_size=32):\n",
    "    num_samples = len(X_train)\n",
    "    for _ in range(num_iterations):\n",
    "        # Sample random indices for mini-batch\n",
    "        batch_indices = np.random.choice(num_samples, size=batch_size, replace=False)\n",
    "        X_batch = X_train[batch_indices]\n",
    "        y_batch = y_train[batch_indices]\n",
    "\n",
    "        # Print loss and learning rate for monitoring\n",
    "        y_pred = np.array([coarse_model(X_batch[i, :], w0, w1, w2) for i in range(len(X_batch))])\n",
    "        loss = loss_function(y_pred, y_batch)\n",
    "\n",
    "        # Compute numerical gradients\n",
    "        gradients = numerical_gradient(X_batch, y_batch,y_pred, w0, w1, w2)\n",
    "\n",
    "        # Update parameters with adaptive learning rate\n",
    "        current_learning_rate = learning_rate / np.sqrt(50*_ + 1)\n",
    "        w0 -= current_learning_rate * np.array(gradients[0])\n",
    "        w1 -= current_learning_rate * np.array(gradients[1])\n",
    "        w2 -= current_learning_rate * np.array(gradients[2])\n",
    "        \n",
    "        # print the information of this iteration\n",
    "        print(\"Iteration {}, Loss: {}, Learning Rate: {}\".format(_, loss, current_learning_rate))\n",
    "        \n",
    "    return w0, w1, w2\n",
    "\n",
    "# Assuming X_train and y_train are already defined\n",
    "# Initialize parameters\n",
    "w0 = np.random.rand(150)\n",
    "w1 = np.random.rand(150)\n",
    "w2 = np.random.rand(150)\n",
    "\n",
    "# Run gradient descent\n",
    "w0_optimized, w1_optimized, w2_optimized = gradient_descent(X_train, y_train, w0_optimized, w1_optimized, w2_optimized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to see if it works  [[3119.4062 3119.4019 3119.3943 ... 3119.442  3119.4377 3119.4333]\n",
      " [3616.5867 3616.581  3616.569  ... 3616.5852 3616.586  3616.5854]\n",
      " [5049.869  5049.8853 5049.9116 ... 5049.896  5049.891  5049.8887]\n",
      " ...\n",
      " [3116.0266 3116.0347 3116.052  ... 3116.043  3116.0415 3116.04  ]\n",
      " [4515.473  4515.487  4515.5117 ... 4515.501  4515.497  4515.4946]\n",
      " [2730.2244 2730.231  2730.245  ... 2730.237  2730.2378 2730.2358]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Gradient only defined for scalar-output functions. Output had shape: (32,).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m y_train \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray(y_train)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Run gradient descent\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m w0_optimized, w1_optimized, w2_optimized \u001b[38;5;241m=\u001b[39m gradient_descent(X_train, y_train, w0, w1, w2)\n",
      "Cell \u001b[0;32mIn[15], line 27\u001b[0m, in \u001b[0;36mgradient_descent\u001b[0;34m(X_train, y_train, w0, w1, w2, learning_rate, num_iterations, batch_size)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to see if it works \u001b[39m\u001b[38;5;124m'\u001b[39m,loss_wrapper_i(np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m150\u001b[39m))) )\n\u001b[1;32m     26\u001b[0m gradient_loss \u001b[38;5;241m=\u001b[39m grad(loss_wrapper_i)\n\u001b[0;32m---> 27\u001b[0m gradients \u001b[38;5;241m=\u001b[39m gradient_loss(params)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Update parameters with adaptive learning rate\u001b[39;00m\n\u001b[1;32m     30\u001b[0m current_learning_rate \u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m/\u001b[39m jnp\u001b[38;5;241m.\u001b[39msqrt(_ \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 4 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/bima/lib/python3.11/site-packages/jax/_src/api.py:741\u001b[0m, in \u001b[0;36m_check_scalar\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(aval, ShapedArray):\n\u001b[1;32m    740\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m aval\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m ():\n\u001b[0;32m--> 741\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhad shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maval\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    742\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    743\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhad abstract value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: Gradient only defined for scalar-output functions. Output had shape: (32,)."
     ]
    }
   ],
   "source": [
    "def loss_function(y_pred, y_true):\n",
    "    # Mean squared error loss\n",
    "    return jnp.mean((y_pred - y_true) ** 2, axis = 1)\n",
    "\n",
    "# Define a wrapped version of the loss function that accepts parameters as a single vector\n",
    "def loss_wrapper(params, X, y):\n",
    "    w0, w1, w2 = jnp.split(params, 3)\n",
    "    y_pred = [ coarse_model(X[i,:], w0, w1, w2) for i in range(len(X[:,0])) ]\n",
    "    return loss_function(jnp.array(y_pred), y)\n",
    "\n",
    "# Compute gradients of the loss function with respect to parameters using JAX's grad function\n",
    "gradient_loss = grad(loss_wrapper)\n",
    "\n",
    "def gradient_descent(X_train, y_train, w0, w1, w2, learning_rate=0.001, num_iterations=100, batch_size=32):\n",
    "    num_samples = len(X_train)\n",
    "    params = jnp.concatenate([w0, w1, w2])  # Combine parameters into a single vector\n",
    "    for _ in range(num_iterations):\n",
    "        # Sample random indices for mini-batch\n",
    "        batch_indices = np.random.choice(num_samples, size=batch_size, replace=False)\n",
    "        X_batch = X_train[batch_indices]\n",
    "        y_batch = y_train[batch_indices]\n",
    "\n",
    "        # Compute numerical gradients using JAX's grad function\n",
    "        loss_wrapper_i = lambda params: loss_wrapper(params, X_batch, y_batch)\n",
    "        print(' to see if it works ',loss_wrapper_i(np.zeros((3,150))) )\n",
    "        gradient_loss = grad(loss_wrapper_i)\n",
    "        gradients = gradient_loss(params)\n",
    "\n",
    "        # Update parameters with adaptive learning rate\n",
    "        current_learning_rate = learning_rate / jnp.sqrt(_ + 1)\n",
    "        params -= current_learning_rate * gradients\n",
    "\n",
    "        # Unpack updated parameters\n",
    "        w0, w1, w2 = jnp.split(params, 3)\n",
    "\n",
    "        # Print loss and learning rate for monitoring\n",
    "        y_pred = np.array([coarse_model(X_train[i, :], w0, w1, w2) for i in range(len(X_train))])\n",
    "        loss = loss_function(y_pred, y_train)\n",
    "        print(\"Iteration {}, Loss: {}, Learning Rate: {}\".format(_, loss, current_learning_rate))\n",
    "        \n",
    "    return w0, w1, w2\n",
    "\n",
    "# Assuming X_train and y_train are already defined\n",
    "# Initialize parameters\n",
    "w0 = jnp.array(np.zeros(150))\n",
    "w1 = jnp.array(np.zeros(150))\n",
    "w2 = jnp.array(np.zeros(150))\n",
    "\n",
    "# Convert X_train and y_train to JAX arrays if not already\n",
    "X_train = jnp.array(X_train)\n",
    "y_train = jnp.array(y_train)\n",
    "\n",
    "# Run gradient descent\n",
    "w0_optimized, w1_optimized, w2_optimized = gradient_descent(X_train, y_train, w0, w1, w2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bima",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
