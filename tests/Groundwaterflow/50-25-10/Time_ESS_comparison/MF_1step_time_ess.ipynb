{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze the TIME / ESS distribution \n",
    "\n",
    "Randomly choose 25 samples and evaluate the model's Time / ESS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-22 16:02:23.655887: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ray module not found. Multiprocessing features are not available\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "\n",
    "import os\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "\n",
    "# Third-party library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "import arviz as az\n",
    "import timeit\n",
    "\n",
    "import scipy.stats as stats\n",
    "from keras.models import Model as Model_nn\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.layers import Input, Dense, Add\n",
    "\n",
    "#Try with TinyDA\n",
    "import tinyDA as tda\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.stats import uniform\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "# Local module imports\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../solver')\n",
    "#sys.path.append('./src/InverseProblems')\n",
    "#sys.path.append('./src/utils')\n",
    "from utils import * \n",
    "from plotting import *\n",
    "from random_process import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the 25 random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([57, 32, 55, 69,  3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 25 \n",
    "np.random.seed(2109)\n",
    "random_samples = np.random.randint(0, 160, n)\n",
    "random_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data and surrogate model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract test data for visualization or further processing\n",
    "n_eig = 64\n",
    "X_values = np.loadtxt('../../data/50-25-10/X_test_50resolution.csv', delimiter = ',')\n",
    "y_values = np.loadtxt('../../data/50-25-10/y_test_50resolution.csv',delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolution parameters\n",
    "resolution_h1 = (50, 50)\n",
    "resolution_h2 = (25, 25)\n",
    "\n",
    "# PDE parameters\n",
    "field_mean = 1\n",
    "field_stdev = 1\n",
    "lamb_cov = 0.1\n",
    "mkl = 64\n",
    "\n",
    "# Set up the model(s)\n",
    "solver_h1 = Model(resolution_h1, field_mean, field_stdev, mkl, lamb_cov)\n",
    "solver_h2 = Model(resolution_h2, field_mean, field_stdev, mkl, lamb_cov)\n",
    "\n",
    "\n",
    "# Adjust the trasmissivity based on h1\n",
    "list1 = solver_h1.solver.mesh.coordinates()\n",
    "list2 = solver_h2.solver.mesh.coordinates()\n",
    "\n",
    "# Convert lists to numpy arrays if they are not already\n",
    "array1 = np.array(list1)\n",
    "array2 = np.array(list2)\n",
    "\n",
    "# Convert to structured arrays for easy row-wise comparison\n",
    "dtype = {'names': ['f{}'.format(i) for i in range(array1.shape[1])],\n",
    "         'formats': [array1.dtype] * array1.shape[1]}\n",
    "\n",
    "structured_array1 = array1.view(dtype)\n",
    "structured_array2 = array2.view(dtype)\n",
    "\n",
    "# Create the boolean vector by checking if each row in array1 is in array2\n",
    "bool_vector2 = np.in1d(structured_array1, structured_array2)\n",
    "\n",
    "# Set the trasmissivity field\n",
    "solver_h2.random_process.eigenvalues = solver_h1.random_process.eigenvalues\n",
    "solver_h2.random_process.eigenvectors = solver_h1.random_process.eigenvectors[bool_vector2]\n",
    "\n",
    "x_data = y_data = np.array([0.1, 0.3, 0.5, 0.7, 0.9])\n",
    "datapoints = np.array(list(product(x_data, y_data)))\n",
    "\n",
    "def solver_h2_data(x):\n",
    "    solver_h2.solve(x)\n",
    "    return solver_h2.get_data(datapoints)\n",
    "\n",
    "\n",
    "# Choose the model parameters \n",
    "n_samples = 16000\n",
    "coeff = 1e-08\n",
    "\n",
    "n_neurons = 256\n",
    "# Initialize the neural network model\n",
    "# Define the three branches of the model\n",
    "input_params = Input(shape=(X_values.shape[1],))\n",
    "input_pod = Input(shape=(y_values.shape[1],))\n",
    "\n",
    "# Define the first branch (parameters)\n",
    "x1 = Dense(n_neurons, activation='gelu')(input_params)\n",
    "x1 = Dense(n_neurons, activation='gelu')(x1)\n",
    "x1 = Dense(n_neurons, activation='gelu')(x1)\n",
    "x1 = Dense(n_neurons, activation='gelu')(x1)\n",
    "x1 = Dense(n_neurons, activation='gelu')(x1)\n",
    "x1 = Dense(n_neurons, activation='gelu')(x1)\n",
    "\n",
    "# Define the second branch (POD)\n",
    "x2 = Dense(n_neurons, activation='gelu')(input_pod)\n",
    "\n",
    "# # Define the second branch (POD)\n",
    "# x3 = Dense(n_neurons, activation='gelu', kernel_regularizer=l2(w))(input_nn)\n",
    "\n",
    "# Combine the outputs of the three branches\n",
    "combined = Add()([x1,x2])\n",
    "combined = Dense(n_neurons, activation='gelu')(combined)\n",
    "output = Dense(25, activation='linear')(combined)\n",
    "\n",
    "# Create the model\n",
    "model_hf = Model_nn(inputs=[input_params,input_pod], outputs=output)\n",
    "model_hf = load_model(f'..//models/model_1step_50-25resolution_{n_samples}samples_1.keras')\n",
    "\n",
    "model_nn = lambda input: model_hf([input.reshape(1,64), solver_h2_data(input).reshape(1,25)]).numpy().reshape(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time / ESS noise 0.001 and multiplicative coefficient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample =  57\n",
      "Sampling chain 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running chain, α = 0.36: 100%|██████████| 1000/1000 [00:16<00:00, 61.71it/s]\n",
      "/Users/lucacaroselli/miniconda3/envs/fenics/lib/python3.11/site-packages/arviz/data/inference_data.py:157: UserWarning: qoi group is not defined in the InferenceData scheme\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 16.236571500019636    ESS:  4.245821284127148    Time/ESS:  3.8241297533458325\n",
      "Sample =  32\n",
      "Sampling chain 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running chain, α = 0.43: 100%|██████████| 1000/1000 [00:18<00:00, 55.27it/s]\n",
      "/Users/lucacaroselli/miniconda3/envs/fenics/lib/python3.11/site-packages/arviz/data/inference_data.py:157: UserWarning: qoi group is not defined in the InferenceData scheme\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 18.110157374991104    ESS:  3.9787764472188263    Time/ESS:  4.551690102531431\n",
      "Sample =  55\n",
      "Sampling chain 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running chain, α = 0.34: 100%|██████████| 1000/1000 [00:16<00:00, 62.13it/s]\n",
      "/Users/lucacaroselli/miniconda3/envs/fenics/lib/python3.11/site-packages/arviz/data/inference_data.py:157: UserWarning: qoi group is not defined in the InferenceData scheme\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 16.11554962498485    ESS:  4.311329857842728    Time/ESS:  3.737953289672118\n",
      "Sample =  69\n",
      "Sampling chain 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running chain, α = 0.38: 100%|██████████| 1000/1000 [00:15<00:00, 64.49it/s]\n",
      "/Users/lucacaroselli/miniconda3/envs/fenics/lib/python3.11/site-packages/arviz/data/inference_data.py:157: UserWarning: qoi group is not defined in the InferenceData scheme\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 15.52463887500926    ESS:  4.680923398510175    Time/ESS:  3.3165761439186072\n",
      "Sample =  3\n",
      "Sampling chain 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running chain, α = 0.35: 100%|██████████| 1000/1000 [00:15<00:00, 66.61it/s]\n",
      "/Users/lucacaroselli/miniconda3/envs/fenics/lib/python3.11/site-packages/arviz/data/inference_data.py:157: UserWarning: qoi group is not defined in the InferenceData scheme\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 15.0324454579968    ESS:  3.975867565394955    Time/ESS:  3.780922078198927\n"
     ]
    }
   ],
   "source": [
    "noise = 0.001\n",
    "scaling = 0.015 #0.05\n",
    "n_iter = 55000\n",
    "burnin = 5000\n",
    "thin = 50\n",
    "\n",
    "Times = []\n",
    "Time_ESS = []\n",
    "ESS = []\n",
    "i = 1\n",
    "\n",
    "# Define the prior distribution and the proposal (common to all samples)\n",
    "x_distribution = stats.multivariate_normal(mean = np.zeros(64), cov = np.eye(64))\n",
    "my_proposal = tda.CrankNicolson(scaling=scaling, adaptive=False, gamma = 1.01, period=100)\n",
    "\n",
    "for sample in random_samples:\n",
    "    print('Sample = ', sample)\n",
    "    x_true = X_values[sample]\n",
    "    y_true = y_values[sample]\n",
    "\n",
    "    y_observed = y_true + np.random.normal(scale=noise,size=y_true.shape[0])\n",
    "\n",
    "    # LIKELYHOOD\n",
    "    cov_likelihood = noise**2 * np.eye(25)\n",
    "    y_distribution = tda.GaussianLogLike(y_observed, cov_likelihood*10)  \n",
    "\n",
    "    # initialise the Posterior\n",
    "    my_posterior = tda.Posterior(x_distribution, y_distribution, model_nn)\n",
    "\n",
    "    # RUN THE MCMC\n",
    "    start = timeit.default_timer()\n",
    "    samples = tda.sample(my_posterior, my_proposal, iterations=n_iter, n_chains=1, initial_parameters=np.zeros(64))\n",
    "    end = timeit.default_timer()\n",
    "\n",
    "    # Remove the burnin and sub-sample\n",
    "    idata = tda.to_inference_data(samples, level='fine')\n",
    "    idata = idata.sel(draw=slice(burnin, None, thin), groups=\"posterior\")\n",
    "    ess = az.ess(idata)\n",
    "\n",
    "    #Compute the time\n",
    "    t = end-start\n",
    "    Times.append(t)\n",
    "\n",
    "    # Compute the mean ESS on the 64 parameters\n",
    "    e = np.mean([ess.data_vars['x'+str(i)].values for i in range(64)])\n",
    "    ESS.append(e)\n",
    "\n",
    "    #Compute Time / ESS\n",
    "    Time_ESS.append(t/e)\n",
    "    \n",
    "    print('Time:', t, '   ESS: ', e, '   Time/ESS: ',t/e )\n",
    "\n",
    "\n",
    "# Save the results \n",
    "# Specify the folder path (assuming it already exists)\n",
    "folder_path = './recorded_values'  # Replace with your actual path\n",
    "\n",
    "# Save the file in the specified folder\n",
    "file_path = os.path.join(folder_path, 'MF_1step_time_ess_001.npy')\n",
    "np.save(file_path, Time_ESS)\n",
    "file_path = os.path.join(folder_path, 'MF_1step_Times_001.npy')\n",
    "np.save(file_path, Times)\n",
    "file_path = os.path.join(folder_path, 'MF_1step_ESS_001.npy')\n",
    "np.save(file_path, ESS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time/ESS Higher Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample =  57\n",
      "Sampling chain 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running chain, α = 0.37: 100%|██████████| 1000/1000 [00:14<00:00, 67.85it/s]\n",
      "/Users/lucacaroselli/miniconda3/envs/fenics/lib/python3.11/site-packages/arviz/data/inference_data.py:157: UserWarning: qoi group is not defined in the InferenceData scheme\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 14.776755625003716    ESS:  5.0205495454204145    Time/ESS:  2.943254616117194       1 / 5\n",
      "Sample =  32\n",
      "Sampling chain 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running chain, α = 0.31: 100%|██████████| 1000/1000 [00:15<00:00, 66.60it/s]\n",
      "/Users/lucacaroselli/miniconda3/envs/fenics/lib/python3.11/site-packages/arviz/data/inference_data.py:157: UserWarning: qoi group is not defined in the InferenceData scheme\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 15.030243749992223    ESS:  4.638714055364616    Time/ESS:  3.2401746627623944       2 / 5\n",
      "Sample =  55\n",
      "Sampling chain 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running chain, α = 0.38: 100%|██████████| 1000/1000 [00:14<00:00, 69.79it/s]\n",
      "/Users/lucacaroselli/miniconda3/envs/fenics/lib/python3.11/site-packages/arviz/data/inference_data.py:157: UserWarning: qoi group is not defined in the InferenceData scheme\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 14.346181083994452    ESS:  5.447396159949864    Time/ESS:  2.633585049214869       3 / 5\n",
      "Sample =  69\n",
      "Sampling chain 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running chain, α = 0.28: 100%|██████████| 1000/1000 [00:15<00:00, 62.94it/s]\n",
      "/Users/lucacaroselli/miniconda3/envs/fenics/lib/python3.11/site-packages/arviz/data/inference_data.py:157: UserWarning: qoi group is not defined in the InferenceData scheme\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 15.903650249994826    ESS:  5.504153573247869    Time/ESS:  2.8893907189094765       4 / 5\n",
      "Sample =  3\n",
      "Sampling chain 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running chain, α = 0.42: 100%|██████████| 1000/1000 [00:17<00:00, 57.57it/s]\n",
      "/Users/lucacaroselli/miniconda3/envs/fenics/lib/python3.11/site-packages/arviz/data/inference_data.py:157: UserWarning: qoi group is not defined in the InferenceData scheme\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 17.388912833004724    ESS:  5.447537120459787    Time/ESS:  3.1920687181177123       5 / 5\n"
     ]
    }
   ],
   "source": [
    "noise = 0.01\n",
    "scaling = 0.04\n",
    "n_iter =  55000\n",
    "burnin = 5000\n",
    "thin = 50\n",
    "\n",
    "Times = []\n",
    "Time_ESS = []\n",
    "ESS = []\n",
    "i = 1\n",
    "\n",
    "# Define the prior distribution and the proposal (common to all samples)\n",
    "x_distribution = stats.multivariate_normal(mean = np.zeros(64), cov = np.eye(64))\n",
    "my_proposal = tda.CrankNicolson(scaling=scaling, adaptive=False, gamma = 1.01, period=100)\n",
    "\n",
    "for sample in random_samples:\n",
    "    print('Sample = ', sample)\n",
    "    x_true = X_values[sample]\n",
    "    y_true = y_values[sample]\n",
    "\n",
    "    y_observed = y_true + np.random.normal(scale=noise,size=y_true.shape[0])\n",
    "\n",
    "    # LIKELYHOOD\n",
    "    cov_likelihood = noise**2 * np.eye(25)\n",
    "    y_distribution = tda.GaussianLogLike(y_observed, cov_likelihood)  \n",
    "\n",
    "    # initialise the Posterior\n",
    "    my_posterior = tda.Posterior(x_distribution, y_distribution, model_nn)\n",
    "\n",
    "    # RUN THE MCMC\n",
    "    start = timeit.default_timer()\n",
    "    samples = tda.sample(my_posterior, my_proposal, iterations=n_iter, n_chains=1, initial_parameters=np.zeros(64))\n",
    "    end = timeit.default_timer()\n",
    "\n",
    "    # Remove the burnin and sub-sample\n",
    "    idata = tda.to_inference_data(samples, level='fine')\n",
    "    idata = idata.sel(draw=slice(burnin, None, thin), groups=\"posterior\")\n",
    "    ess = az.ess(idata)\n",
    "\n",
    "    #Compute the time\n",
    "    t = end-start\n",
    "    Times.append(t)\n",
    "\n",
    "    # Compute the mean ESS on the 64 parameters\n",
    "    e = np.mean([ess.data_vars['x'+str(i)].values for i in range(64)])\n",
    "    ESS.append(e)\n",
    "\n",
    "    #Compute Time / ESS\n",
    "    Time_ESS.append(t/e)\n",
    "    \n",
    "    print('Time:', t, '   ESS: ', e, '   Time/ESS: ',t/e , '     ', i,'/', len(random_samples))\n",
    "\n",
    "    i = i+1\n",
    "\n",
    "# Save the results \n",
    "# Specify the folder path (assuming it already exists)\n",
    "folder_path = './recorded_values'  # Replace with your actual path\n",
    "\n",
    "# Save the file in the specified folder\n",
    "file_path = os.path.join(folder_path, 'MF_1step_time_ess_01.npy')\n",
    "np.save(file_path, Time_ESS)\n",
    "file_path = os.path.join(folder_path, 'MF_1step_Times_01.npy')\n",
    "np.save(file_path, Times)\n",
    "file_path = os.path.join(folder_path, 'MF_1step_ESS_01.npy')\n",
    "np.save(file_path, ESS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fenics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
