{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze the TIME / ESS distribution \n",
    "\n",
    "Randomly choose 25 samples and evaluate the model's Time / ESS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "\n",
    "import os\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "\n",
    "# Third-party library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "import arviz as az\n",
    "import timeit\n",
    "\n",
    "import scipy.stats as stats\n",
    "from keras.models import Model as Model_nn\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.layers import Input, Dense, Add\n",
    "\n",
    "#Try with TinyDA\n",
    "import tinyDA as tda\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.stats import uniform\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "# Local module imports\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../solver')\n",
    "#sys.path.append('./src/InverseProblems')\n",
    "#sys.path.append('./src/utils')\n",
    "from utils import * \n",
    "from plotting import *\n",
    "from random_process import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the 25 random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([57, 32, 55, 69,  3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 25 \n",
    "np.random.seed(2109)\n",
    "random_samples = np.random.randint(0, 160, n)\n",
    "random_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data and surrogate model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract test data for visualization or further processing\n",
    "n_eig = 64\n",
    "X_values = np.loadtxt('../../data/50-25-10/X_test_50resolution.csv', delimiter = ',')\n",
    "y_values = np.loadtxt('../../data/50-25-10/y_test_50resolution.csv',delimiter = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Low fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the model parameters \n",
    "n_samples_lf = 16000\n",
    "coeff_lf = 1e-08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucacaroselli/miniconda3/envs/fenics/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the neural network model\n",
    "model_l = Sequential([\n",
    "    Dense(256, input_shape=(X_values.shape[1],), activation='gelu'),\n",
    "    Dense(256, activation='gelu'),\n",
    "    Dense(256, activation='gelu'),\n",
    "    Dense(256, activation='gelu'),\n",
    "    Dense(256, activation='gelu'),\n",
    "    Dense(256, activation='gelu'),\n",
    "    Dense(25, activation='linear')\n",
    "])\n",
    "\n",
    "model_l = load_model(f'../models/model_50resolution_{n_samples_lf}samples_1.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load High Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolution parameters\n",
    "resolution_h1 = (50, 50)\n",
    "resolution_h2 = (25, 25)\n",
    "\n",
    "# PDE parameters\n",
    "field_mean = 1\n",
    "field_stdev = 1\n",
    "lamb_cov = 0.1\n",
    "mkl = 64\n",
    "\n",
    "# Set up the model(s)\n",
    "solver_h1 = Model(resolution_h1, field_mean, field_stdev, mkl, lamb_cov)\n",
    "solver_h2 = Model(resolution_h2, field_mean, field_stdev, mkl, lamb_cov)\n",
    "\n",
    "\n",
    "# Adjust the trasmissivity based on h1\n",
    "list1 = solver_h1.solver.mesh.coordinates()\n",
    "list2 = solver_h2.solver.mesh.coordinates()\n",
    "\n",
    "# Convert lists to numpy arrays if they are not already\n",
    "array1 = np.array(list1)\n",
    "array2 = np.array(list2)\n",
    "\n",
    "# Convert to structured arrays for easy row-wise comparison\n",
    "dtype = {'names': ['f{}'.format(i) for i in range(array1.shape[1])],\n",
    "         'formats': [array1.dtype] * array1.shape[1]}\n",
    "\n",
    "structured_array1 = array1.view(dtype)\n",
    "structured_array2 = array2.view(dtype)\n",
    "\n",
    "# Create the boolean vector by checking if each row in array1 is in array2\n",
    "bool_vector2 = np.in1d(structured_array1, structured_array2)\n",
    "\n",
    "# Set the trasmissivity field\n",
    "solver_h2.random_process.eigenvalues = solver_h1.random_process.eigenvalues\n",
    "solver_h2.random_process.eigenvectors = solver_h1.random_process.eigenvectors[bool_vector2]\n",
    "\n",
    "x_data = y_data = np.array([0.1, 0.3, 0.5, 0.7, 0.9])\n",
    "datapoints = np.array(list(product(x_data, y_data)))\n",
    "\n",
    "def solver_h2_data(x):\n",
    "    solver_h2.solve(x)\n",
    "    return solver_h2.get_data(datapoints)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Choose the model parameters \n",
    "n_samples = 16000\n",
    "coeff = 1e-08\n",
    "\n",
    "n_neurons = 256\n",
    "# Initialize the neural network model\n",
    "# Define the three branches of the model\n",
    "input_params = Input(shape=(X_values.shape[1],))\n",
    "input_pod = Input(shape=(y_values.shape[1],))\n",
    "\n",
    "# Define the first branch (parameters)\n",
    "x1 = Dense(n_neurons, activation='gelu')(input_params)\n",
    "x1 = Dense(n_neurons, activation='gelu')(x1)\n",
    "x1 = Dense(n_neurons, activation='gelu')(x1)\n",
    "x1 = Dense(n_neurons, activation='gelu')(x1)\n",
    "x1 = Dense(n_neurons, activation='gelu')(x1)\n",
    "x1 = Dense(n_neurons, activation='gelu')(x1)\n",
    "\n",
    "# Define the second branch (POD)\n",
    "x2 = Dense(n_neurons, activation='gelu')(input_pod)\n",
    "\n",
    "# # Define the second branch (POD)\n",
    "# x3 = Dense(n_neurons, activation='gelu', kernel_regularizer=l2(w))(input_nn)\n",
    "\n",
    "# Combine the outputs of the three branches\n",
    "combined = Add()([x1,x2])\n",
    "combined = Dense(n_neurons, activation='gelu')(combined)\n",
    "output = Dense(25, activation='linear')(combined)\n",
    "\n",
    "# Create the model\n",
    "model_h = Model_nn(inputs=[input_params,input_pod], outputs=output)\n",
    "model_h = load_model(f'..//models/model_1step_50-25resolution_{n_samples}samples_1.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lf = lambda input : model_l(input.reshape(1,64)).numpy().reshape(25) \n",
    "model_hf = lambda input: model_h([input.reshape(1,64), solver_h2_data(input).reshape(1,25)]).numpy().reshape(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time / ESS noise 0.001 and multiplicative coefficient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample =  57\n",
      "Sampling chain 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running chain, α_c = 0.392, α_f = 0.33:   5%|▍         | 47/1000 [00:00<00:13, 71.42it/s]/Users/lucacaroselli/miniconda3/envs/fenics/lib/python3.11/site-packages/tinyDA/proposal.py:340: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(proposal_link.likelihood - previous_link.likelihood)\n",
      "Running chain, α_c = 0.430, α_f = 0.34: 100%|██████████| 1000/1000 [00:11<00:00, 84.38it/s]\n",
      "/Users/lucacaroselli/miniconda3/envs/fenics/lib/python3.11/site-packages/arviz/data/inference_data.py:157: UserWarning: qoi group is not defined in the InferenceData scheme\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 11.968942583014723    ESS:  4.122430318234322    Time/ESS:  2.9033705021219474\n",
      "Sample =  32\n",
      "Sampling chain 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running chain, α_c = 0.360, α_f = 0.32: 100%|██████████| 1000/1000 [00:11<00:00, 86.75it/s]\n",
      "/Users/lucacaroselli/miniconda3/envs/fenics/lib/python3.11/site-packages/arviz/data/inference_data.py:157: UserWarning: qoi group is not defined in the InferenceData scheme\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 11.54722033298458    ESS:  3.802613454946228    Time/ESS:  3.0366537303350145\n",
      "Sample =  55\n",
      "Sampling chain 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running chain, α_c = 0.280, α_f = 0.16: 100%|██████████| 1000/1000 [00:11<00:00, 84.70it/s]\n",
      "/Users/lucacaroselli/miniconda3/envs/fenics/lib/python3.11/site-packages/arviz/data/inference_data.py:157: UserWarning: qoi group is not defined in the InferenceData scheme\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 11.826589790987782    ESS:  3.285221425006511    Time/ESS:  3.5999368873482687\n",
      "Sample =  69\n",
      "Sampling chain 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running chain, α_c = 0.250, α_f = 0.27: 100%|██████████| 1000/1000 [00:11<00:00, 84.71it/s]\n",
      "/Users/lucacaroselli/miniconda3/envs/fenics/lib/python3.11/site-packages/arviz/data/inference_data.py:157: UserWarning: qoi group is not defined in the InferenceData scheme\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 11.825682333990699    ESS:  4.046235431941922    Time/ESS:  2.922638223331252\n",
      "Sample =  3\n",
      "Sampling chain 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running chain, α_c = 0.300, α_f = 0.27: 100%|██████████| 1000/1000 [00:11<00:00, 85.86it/s]\n",
      "/Users/lucacaroselli/miniconda3/envs/fenics/lib/python3.11/site-packages/arviz/data/inference_data.py:157: UserWarning: qoi group is not defined in the InferenceData scheme\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 11.66707204200793    ESS:  4.08602384232819    Time/ESS:  2.855360735085703\n"
     ]
    }
   ],
   "source": [
    "noise = 0.001\n",
    "scaling = 0.015 # 0.04\n",
    "n_iter =  33000 #55000\n",
    "burnin = 3000 #5000\n",
    "thin = 20\n",
    "sub_sampling = 1\n",
    "\n",
    "Times = []\n",
    "Time_ESS = []\n",
    "ESS = []\n",
    "i = 1\n",
    "\n",
    "# Define the prior distribution and the proposal (common to all samples)\n",
    "x_distribution = stats.multivariate_normal(mean = np.zeros(64), cov = np.eye(64))\n",
    "my_proposal = tda.CrankNicolson(scaling=scaling, adaptive=False, gamma = 1.01, period=100)\n",
    "\n",
    "for sample in random_samples:\n",
    "    print('Sample = ', sample)\n",
    "    x_true = X_values[sample]\n",
    "    y_true = y_values[sample]\n",
    "\n",
    "    y_observed = y_true + np.random.normal(scale=noise,size=y_true.shape[0])\n",
    "\n",
    "    # LIKELYHOOD\n",
    "    cov_likelihood = noise**2 * np.eye(25)\n",
    "    y_distribution_coarse = tda.AdaptiveGaussianLogLike(y_observed, cov_likelihood*10)\n",
    "    y_distribution_fine  = tda.GaussianLogLike(y_observed, cov_likelihood*10)\n",
    "\n",
    "    # initialise the Posterior\n",
    "    my_posterior_coarse = tda.Posterior(x_distribution, y_distribution_coarse, model_lf)\n",
    "    my_posterior_fine = tda.Posterior(x_distribution, y_distribution_fine, model_hf)\n",
    "    my_posteriors = [my_posterior_coarse, my_posterior_fine]\n",
    "\n",
    "    # RUN THE MCMC\n",
    "    start = timeit.default_timer()\n",
    "    samples = tda.sample(my_posteriors, my_proposal, iterations=n_iter, n_chains=1, initial_parameters=np.zeros(64), subsampling_rate= sub_sampling, adaptive_error_model='state-independent')\n",
    "    end = timeit.default_timer()\n",
    "\n",
    "    # Remove the burnin and sub-sample\n",
    "    idata = tda.to_inference_data(samples, level='fine')\n",
    "    idata = idata.sel(draw=slice(burnin, None, thin), groups=\"posterior\")\n",
    "    ess = az.ess(idata)\n",
    "\n",
    "    #Compute the time\n",
    "    t = end-start\n",
    "    Times.append(t)\n",
    "\n",
    "    # Compute the mean ESS on the 64 parameters\n",
    "    e = np.mean([ess.data_vars['x'+str(i)].values for i in range(64)])\n",
    "    ESS.append(e)\n",
    "\n",
    "    #Compute Time / ESS\n",
    "    Time_ESS.append(t/e)\n",
    "    \n",
    "    print('Time:', t, '   ESS: ', e, '   Time/ESS: ',t/e )\n",
    "\n",
    "# Save the results \n",
    "# Specify the folder path (assuming it already exists)\n",
    "folder_path = './recorded_values'  # Replace with your actual path\n",
    "\n",
    "# Save the file in the specified folder\n",
    "file_path = os.path.join(folder_path, 'MDA_MF_1step_time_ess_001.npy')\n",
    "np.save(file_path, Time_ESS)\n",
    "file_path = os.path.join(folder_path, 'MDA_MF_1step_Times_001.npy')\n",
    "np.save(file_path, Times)\n",
    "file_path = os.path.join(folder_path, 'MDA_MF_1step_ESS_001.npy')\n",
    "np.save(file_path, ESS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time/ESS Higher Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample =  57\n",
      "Sampling chain 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running chain, α_c = 0.270, α_f = 0.26: 100%|██████████| 1000/1000 [00:11<00:00, 86.17it/s]\n",
      "/Users/lucacaroselli/miniconda3/envs/fenics/lib/python3.11/site-packages/arviz/data/inference_data.py:157: UserWarning: qoi group is not defined in the InferenceData scheme\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 11.646104250015924    ESS:  3.2757094966013027    Time/ESS:  3.555292147273525       1 / 5\n",
      "Sample =  32\n",
      "Sampling chain 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running chain, α_c = 0.430, α_f = 0.34: 100%|██████████| 1000/1000 [00:11<00:00, 87.44it/s]\n",
      "/Users/lucacaroselli/miniconda3/envs/fenics/lib/python3.11/site-packages/arviz/data/inference_data.py:157: UserWarning: qoi group is not defined in the InferenceData scheme\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 11.464480708003975    ESS:  3.7996420593918088    Time/ESS:  3.0172528171874804       2 / 5\n",
      "Sample =  55\n",
      "Sampling chain 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running chain, α_c = 0.370, α_f = 0.36: 100%|██████████| 1000/1000 [00:11<00:00, 89.50it/s]\n",
      "/Users/lucacaroselli/miniconda3/envs/fenics/lib/python3.11/site-packages/arviz/data/inference_data.py:157: UserWarning: qoi group is not defined in the InferenceData scheme\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 11.194468625006266    ESS:  4.506571962232604    Time/ESS:  2.4840319246695013       3 / 5\n",
      "Sample =  69\n",
      "Sampling chain 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running chain, α_c = 0.280, α_f = 0.32: 100%|██████████| 1000/1000 [00:11<00:00, 90.14it/s]\n",
      "/Users/lucacaroselli/miniconda3/envs/fenics/lib/python3.11/site-packages/arviz/data/inference_data.py:157: UserWarning: qoi group is not defined in the InferenceData scheme\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 11.115925582998898    ESS:  4.284278741522855    Time/ESS:  2.594585052383338       4 / 5\n",
      "Sample =  3\n",
      "Sampling chain 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running chain, α_c = 0.290, α_f = 0.30: 100%|██████████| 1000/1000 [00:11<00:00, 87.84it/s]\n",
      "/Users/lucacaroselli/miniconda3/envs/fenics/lib/python3.11/site-packages/arviz/data/inference_data.py:157: UserWarning: qoi group is not defined in the InferenceData scheme\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 11.413445749989478    ESS:  4.533738802092797    Time/ESS:  2.517446692059316       5 / 5\n"
     ]
    }
   ],
   "source": [
    "noise = 0.01\n",
    "scaling = 0.04\n",
    "n_iter =  33000 #55000\n",
    "burnin = 3000 #5000\n",
    "thin = 20\n",
    "sub_sampling = 1\n",
    "\n",
    "Times = []\n",
    "Time_ESS = []\n",
    "ESS = []\n",
    "i = 1\n",
    "\n",
    "# Define the prior distribution and the proposal (common to all samples)\n",
    "x_distribution = stats.multivariate_normal(mean = np.zeros(64), cov = np.eye(64))\n",
    "my_proposal = tda.CrankNicolson(scaling=scaling, adaptive=False, gamma = 1.01, period=100)\n",
    "\n",
    "for sample in random_samples:\n",
    "    print('Sample = ', sample)\n",
    "    x_true = X_values[sample] \n",
    "    y_true = y_values[sample]\n",
    "\n",
    "    y_observed = y_true + np.random.normal(scale=noise,size=y_true.shape[0])\n",
    "\n",
    "    # LIKELYHOOD\n",
    "    cov_likelihood = noise**2 * np.eye(25)\n",
    "    y_distribution_coarse = tda.AdaptiveGaussianLogLike(y_observed, cov_likelihood)\n",
    "    y_distribution_fine  = tda.GaussianLogLike(y_observed, cov_likelihood)\n",
    "    # initialise the Posterior\n",
    "    my_posterior_coarse = tda.Posterior(x_distribution, y_distribution_coarse, model_lf)\n",
    "    my_posterior_fine = tda.Posterior(x_distribution, y_distribution_fine, model_hf)\n",
    "    my_posteriors = [my_posterior_coarse, my_posterior_fine]\n",
    "\n",
    "    # RUN THE MCMC\n",
    "    start = timeit.default_timer()\n",
    "    samples = tda.sample(my_posteriors, my_proposal, iterations=n_iter, n_chains=1, initial_parameters=np.zeros(64), subsampling_rate= sub_sampling, adaptive_error_model='state-independent')\n",
    "    end = timeit.default_timer()\n",
    "\n",
    "    # Remove the burnin and sub-sample\n",
    "    idata = tda.to_inference_data(samples, level='fine')\n",
    "    idata = idata.sel(draw=slice(burnin, None, thin), groups=\"posterior\")\n",
    "    ess = az.ess(idata)\n",
    "\n",
    "    #Compute the time\n",
    "    t = end-start\n",
    "    Times.append(t)\n",
    "\n",
    "    # Compute the mean ESS on the 64 parameters\n",
    "    e = np.mean([ess.data_vars['x'+str(i)].values for i in range(64)])\n",
    "    ESS.append(e)\n",
    "\n",
    "    #Compute Time / ESS\n",
    "    Time_ESS.append(t/e)\n",
    "    \n",
    "    print('Time:', t, '   ESS: ', e, '   Time/ESS: ',t/e , '     ', i,'/', len(random_samples))\n",
    "\n",
    "    i = i+1\n",
    "\n",
    "# Save the results \n",
    "# Specify the folder path (assuming it already exists)\n",
    "folder_path = './recorded_values'  # Replace with your actual path\n",
    "\n",
    "# Save the file in the specified folder\n",
    "file_path = os.path.join(folder_path, 'MDA_MF_1step_time_ess_01.npy')\n",
    "np.save(file_path, Time_ESS)\n",
    "file_path = os.path.join(folder_path, 'MDA_MF_1step_Times_01.npy')\n",
    "np.save(file_path, Times)\n",
    "file_path = os.path.join(folder_path, 'MDA_MF_1step_ESS_01.npy')\n",
    "np.save(file_path, ESS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fenics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
