{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ContiPaolo/Multifidelity-Tutorial/blob/main/MF_POD_Burger's1D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXejdLL8NbTY"
      },
      "source": [
        "# **Multi-fidelity reduced-order modeling on Groundwater's flow - Darcy equation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNXEcOy7YdZp"
      },
      "source": [
        "As seen in the identification with the FOM model the nature of the problem poses a limitation on the accuracy of recontruction.\n",
        "It is shown that any model with an accuracy higher that 0.01 will be masked by the noise and, more importantly, by the likelyhood. This is due to the need to explore a high dimentional space (n = 64)\n",
        "\n",
        "We put ourself in the context of a FOM model that is extremely expensive. It would be therefore usefull to be able to retrieve the needed accuracy of the model using the least possibile training data. \n",
        "\n",
        "We aim to create a system with two levels of fidelity using 16000 data fine. \n",
        "\n",
        "The first level will be a Neural Network that, recieving as inputs the 64 coefficients of the trasmissivity field, tries to predict the 25 output values \n",
        "\n",
        "The second level of fidelity is intended as a refinement of the first model. Indeed it recieved as inputs the paramaters and the 25 coarse solution to predict the fine 25 solution "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNpr9aLtZ8bU"
      },
      "source": [
        "#### (1) **Importing libraries and Loading data** \n",
        "#### (2) **Training of Coarse level surrogate model**\n",
        "#### (3) **Generation of multi-fidelity dataset**\n",
        "#### (4) **Training Fine level neural network surrogate model**\n",
        "#### (5) **Evaluation of different models**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WP3luMxfaY7N"
      },
      "source": [
        "### 1 ) Libraries and Data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "dUChnEGpNaQE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "#import matplotlib as plt\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd \n",
        "from tensorflow.keras.optimizers import Adam,Nadam,Adamax\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.models import load_model\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import extmath\n",
        "\n",
        "#######################     CONFIGURATIONS     ##########################\n",
        "seed = 29\n",
        "train = True\n",
        "save = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Choose the number of data used to train the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "N_c = 20000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test = np.loadtxt(\"./data/X_test_coarse_32000.csv\" , delimiter = \",\")[:2000]\n",
        "y_test = np.loadtxt(\"./data/y_test_coarse_32000.csv\" , delimiter = \",\")[:2000]\n",
        "\n",
        "X_train =np.loadtxt(\"./data/X_train_coarse_32000.csv\" , delimiter = \",\")[:N_c]\n",
        "y_train =np.loadtxt(\"./data/y_train_coarse_32000.csv\" , delimiter = \",\")[:N_c]\n",
        "\n",
        "n_c = X_train.shape[1]\n",
        "n_c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset has been developed using fenics library, it is composed of 64000 samples divided in 90% training and 10% testing.\\\n",
        "The input (or X) are the 64 first component of the Karhunen-Loève (KL) decompositions. \\\n",
        "These eigenmodes allow to paratetrize a random field in the most accurate way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The output: 25 sensors are used to record the hydraulic pressure in the domain $\\Omega$ = [ 0 , 1 ] x [ 0 , 1 ]  \\\n",
        "The sensors are distributed on a grid with positions [0.1, 0.3, 0.5, 0.7, 0.9]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2 ) Training Fully connected neural network surrogate model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us try to create a map $T(x) \\rightarrow u(x)$ \n",
        "\n",
        "Create a NN that takes in inuput the 64 eigenvalues of the transittivity field and returns the hydraulic pressure in the 25 points where the sensors are located \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/lucacaroselli/miniconda3/envs/myenv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "Training = False\n",
        "\n",
        "# Define the learning rate scheduler function\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch < 10:\n",
        "        return lr\n",
        "    else:\n",
        "        return max(lr * 0.99, 5e-5)\n",
        "\n",
        "# Initialize the neural network model\n",
        "model_lf = Sequential()\n",
        "\n",
        "\n",
        "# Add layers to the model\n",
        "\n",
        "model_lf.add(Dense(64, input_shape=(n_c,), activation='gelu'))\n",
        "model_lf.add(Dense(64, activation='gelu'))\n",
        "model_lf.add(Dense(64, activation='sigmoid'))\n",
        "model_lf.add(Dense(32, activation='sigmoid'))\n",
        "model_lf.add(Dense(25, activation='exponential'))\n",
        "\n",
        "\n",
        "if Training:\n",
        "    # Compile the model\n",
        "    initial_learning_rate = 0.001\n",
        "    optimizer = Adam(learning_rate=initial_learning_rate)\n",
        "    model_lf.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "\n",
        "    # Define the learning rate scheduler callback\n",
        "    lr_scheduler = LearningRateScheduler(scheduler)\n",
        "\n",
        "    # Train the model\n",
        "    history_lf = model_lf.fit(X_train, y_train, \n",
        "                    epochs=1500, \n",
        "                    batch_size=64, \n",
        "                    validation_data=(X_test, y_test),\n",
        "                    callbacks=[lr_scheduler])\n",
        "    \n",
        "    model_lf.save('./models2/model_LF_20000_2.keras')\n",
        "\n",
        "model_lf = load_model('./models2/model_LF_20000_2.keras')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 615us/step - loss: 1.0536e-04\n",
            "Test accuracy: 0.00010331796511309221\n",
            "Test rmse: 0.010164544510852034\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss = model_lf.evaluate(x=X_test, y=y_test)\n",
        "\n",
        "# Print the test accuracy\n",
        "print(f\"Test accuracy: {test_loss}\")\n",
        "print(f\"Test rmse: {np.sqrt(test_loss)}\")\n",
        "\n",
        "if Training:\n",
        "    plt.plot(history_lf.history['loss'], label='Loss')\n",
        "    plt.plot(history_lf.history['val_loss'], label='Val loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Learning Rate')\n",
        "    plt.title('Learning Rate over Epochs')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se how it is able to prodict "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x1200 with 18 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Define the sampling points.\n",
        "x_data = y_data = np.array([0.1, 0.3, 0.5, 0.7, 0.9])\n",
        "# Grid to plot the basis \n",
        "X,Y = np.meshgrid(x_data, y_data)\n",
        "\n",
        "samples = [ 1, 10, 100]\n",
        "#Plot POD coefficients: LF vs HF\n",
        "fig = plt.figure(figsize=(12,12))\n",
        "plt.subplots_adjust(hspace=0.5)\n",
        "title = 'True vs Reconstucted signals with NN'\n",
        "fig.suptitle(title, fontsize=14)\n",
        "\n",
        "for mode in range(3):\n",
        "    ax = fig.add_subplot(331 + mode)\n",
        "    pcm = plt.pcolormesh(X, Y, y_test[mode, :].reshape((5, 5)))\n",
        "    ax.title.set_text('True signal sample: ' + str(samples[mode]))\n",
        "    plt.colorbar(pcm, ax=ax)\n",
        "    \n",
        "    ax = fig.add_subplot(331 + mode + 3)\n",
        "    reconstructed_sample = np.array(model_lf(X_test[mode, :].reshape((1, n_c)))).reshape((5, 5))\n",
        "    err = y_test[mode, :].reshape(5, 5) - reconstructed_sample\n",
        "    pcm = plt.pcolormesh(X, Y, reconstructed_sample.reshape((5, 5)))\n",
        "    ax.title.set_text('NN prediction sample: ' + str(samples[mode]))\n",
        "    plt.colorbar(pcm, ax=ax)\n",
        "    \n",
        "    ax = fig.add_subplot(331 + mode + 6)\n",
        "    pcm = plt.pcolormesh(X, Y, err)\n",
        "    ax.title.set_text('Reconst. error sample: ' + str(samples[mode]))\n",
        "    plt.colorbar(pcm, ax=ax)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3 ) Generation of multi-fidelity datset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From now on we will only consider the best low fidelity model = Low fidelity 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "N_f = 10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_test_f = np.loadtxt(\"./data/X_test_64000.csv\" , delimiter = \",\")[:2000]\n",
        "y_test_f = np.loadtxt(\"./data/y_test_64000.csv\" , delimiter = \",\")[:2000]\n",
        "\n",
        "X_train_f =np.loadtxt(\"./data/X_train_64000.csv\" , delimiter = \",\")[:N_f]\n",
        "y_train_f =np.loadtxt(\"./data/y_train_64000.csv\" , delimiter = \",\")[:N_f]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "Training = True \n",
        "if Training:\n",
        "    coarse_sol = np.zeros((N_f, 25))\n",
        "    for i in range(N_f):\n",
        "        coarse_sol[i,:] = model_lf(X_train_f[i,:].reshape((1,64)))\n",
        "    \n",
        "    coarse_sol_test = np.zeros((2000,25))\n",
        "    for i in range(2000):\n",
        "        coarse_sol_test[i,:] = model_lf(X_test_f[i,:].reshape((1,64)))\n",
        "    np.savetxt('./data/coarse_sol_training2.csv', coarse_sol, delimiter=',' )\n",
        "    np.savetxt('./data/coarse_sol_test2.csv', coarse_sol_test, delimiter=',' )\n",
        "\n",
        "else: \n",
        "    coarse_sol = np.loadtxt('./data/coarse_sol_training2.csv', delimiter=',')\n",
        "    coarse_sol_test = np.loadtxt('./data/coarse_sol_test2.csv', delimiter=',')\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train2 = np.hstack((X_train_f, coarse_sol))\n",
        "X_test2 = np.hstack((X_test_f, coarse_sol_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4 ) Training Fine-level NN surrogate "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "89"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n_f = X_train2.shape[1]\n",
        "n_f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1113 - val_loss: 0.0056 - learning_rate: 0.0010\n",
            "Epoch 2/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0048 - val_loss: 0.0030 - learning_rate: 0.0010\n",
            "Epoch 3/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0027 - val_loss: 0.0021 - learning_rate: 0.0010\n",
            "Epoch 4/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0019 - val_loss: 0.0017 - learning_rate: 0.0010\n",
            "Epoch 5/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0016 - val_loss: 0.0014 - learning_rate: 0.0010\n",
            "Epoch 6/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0013 - val_loss: 0.0012 - learning_rate: 0.0010\n",
            "Epoch 7/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0012 - val_loss: 0.0011 - learning_rate: 0.0010\n",
            "Epoch 8/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0010 - val_loss: 0.0010 - learning_rate: 0.0010\n",
            "Epoch 9/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.2648e-04 - val_loss: 9.2409e-04 - learning_rate: 0.0010\n",
            "Epoch 10/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 8.4384e-04 - val_loss: 8.3205e-04 - learning_rate: 0.0010\n",
            "Epoch 11/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.6695e-04 - val_loss: 7.7681e-04 - learning_rate: 0.0010\n",
            "Epoch 12/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.9432e-04 - val_loss: 7.5797e-04 - learning_rate: 0.0010\n",
            "Epoch 13/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.4929e-04 - val_loss: 6.6084e-04 - learning_rate: 0.0010\n",
            "Epoch 14/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.8447e-04 - val_loss: 5.9652e-04 - learning_rate: 0.0010\n",
            "Epoch 15/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.3904e-04 - val_loss: 5.9052e-04 - learning_rate: 0.0010\n",
            "Epoch 16/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0066e-04 - val_loss: 5.2183e-04 - learning_rate: 0.0010\n",
            "Epoch 17/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6672e-04 - val_loss: 4.8643e-04 - learning_rate: 0.0010\n",
            "Epoch 18/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3054e-04 - val_loss: 4.9786e-04 - learning_rate: 0.0010\n",
            "Epoch 19/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0486e-04 - val_loss: 5.1644e-04 - learning_rate: 0.0010\n",
            "Epoch 20/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.9372e-04 - val_loss: 4.2195e-04 - learning_rate: 0.0010\n",
            "Epoch 21/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.6999e-04 - val_loss: 4.0214e-04 - learning_rate: 9.9000e-04\n",
            "Epoch 22/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.4620e-04 - val_loss: 3.9938e-04 - learning_rate: 9.8010e-04\n",
            "Epoch 23/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.3127e-04 - val_loss: 3.7896e-04 - learning_rate: 9.7030e-04\n",
            "Epoch 24/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.1513e-04 - val_loss: 3.9600e-04 - learning_rate: 9.6060e-04\n",
            "Epoch 25/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0413e-04 - val_loss: 3.5350e-04 - learning_rate: 9.5099e-04\n",
            "Epoch 26/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9454e-04 - val_loss: 3.3747e-04 - learning_rate: 9.4148e-04\n",
            "Epoch 27/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.7992e-04 - val_loss: 3.3304e-04 - learning_rate: 9.3207e-04\n",
            "Epoch 28/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6754e-04 - val_loss: 3.1746e-04 - learning_rate: 9.2274e-04\n",
            "Epoch 29/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6228e-04 - val_loss: 3.2405e-04 - learning_rate: 9.1352e-04\n",
            "Epoch 30/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5018e-04 - val_loss: 3.0511e-04 - learning_rate: 9.0438e-04\n",
            "Epoch 31/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.4509e-04 - val_loss: 3.0458e-04 - learning_rate: 8.9534e-04\n",
            "Epoch 32/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4326e-04 - val_loss: 3.1453e-04 - learning_rate: 8.8638e-04\n",
            "Epoch 33/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.3077e-04 - val_loss: 3.0716e-04 - learning_rate: 8.7752e-04\n",
            "Epoch 34/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.3404e-04 - val_loss: 2.6497e-04 - learning_rate: 8.6875e-04\n",
            "Epoch 35/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.1464e-04 - val_loss: 2.9588e-04 - learning_rate: 8.6006e-04\n",
            "Epoch 36/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.1290e-04 - val_loss: 2.6933e-04 - learning_rate: 8.5146e-04\n",
            "Epoch 37/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.1840e-04 - val_loss: 2.6497e-04 - learning_rate: 8.4294e-04\n",
            "Epoch 38/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.9978e-04 - val_loss: 2.6242e-04 - learning_rate: 8.3451e-04\n",
            "Epoch 39/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.9368e-04 - val_loss: 2.5010e-04 - learning_rate: 8.2617e-04\n",
            "Epoch 40/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.0456e-04 - val_loss: 2.5302e-04 - learning_rate: 8.1791e-04\n",
            "Epoch 41/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.8776e-04 - val_loss: 2.4114e-04 - learning_rate: 8.0973e-04\n",
            "Epoch 42/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.8446e-04 - val_loss: 2.6777e-04 - learning_rate: 8.0163e-04\n",
            "Epoch 43/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.9106e-04 - val_loss: 2.2527e-04 - learning_rate: 7.9361e-04\n",
            "Epoch 44/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.7402e-04 - val_loss: 2.2761e-04 - learning_rate: 7.8568e-04\n",
            "Epoch 45/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.7450e-04 - val_loss: 2.0952e-04 - learning_rate: 7.7782e-04\n",
            "Epoch 46/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.7264e-04 - val_loss: 2.3936e-04 - learning_rate: 7.7004e-04\n",
            "Epoch 47/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.7472e-04 - val_loss: 2.2788e-04 - learning_rate: 7.6234e-04\n",
            "Epoch 48/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.6347e-04 - val_loss: 2.0551e-04 - learning_rate: 7.5472e-04\n",
            "Epoch 49/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.5986e-04 - val_loss: 2.0354e-04 - learning_rate: 7.4717e-04\n",
            "Epoch 50/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.6276e-04 - val_loss: 2.1079e-04 - learning_rate: 7.3970e-04\n",
            "Epoch 51/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.5590e-04 - val_loss: 2.1993e-04 - learning_rate: 7.3230e-04\n",
            "Epoch 52/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.5593e-04 - val_loss: 2.1743e-04 - learning_rate: 7.2498e-04\n",
            "Epoch 53/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.5366e-04 - val_loss: 2.0864e-04 - learning_rate: 7.1773e-04\n",
            "Epoch 54/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.5252e-04 - val_loss: 1.9391e-04 - learning_rate: 7.1055e-04\n",
            "Epoch 55/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.5220e-04 - val_loss: 1.9496e-04 - learning_rate: 7.0345e-04\n",
            "Epoch 56/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.4252e-04 - val_loss: 2.1249e-04 - learning_rate: 6.9641e-04\n",
            "Epoch 57/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.4311e-04 - val_loss: 2.2263e-04 - learning_rate: 6.8945e-04\n",
            "Epoch 58/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.4867e-04 - val_loss: 1.9029e-04 - learning_rate: 6.8255e-04\n",
            "Epoch 59/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.3889e-04 - val_loss: 2.2177e-04 - learning_rate: 6.7573e-04\n",
            "Epoch 60/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.4322e-04 - val_loss: 1.8335e-04 - learning_rate: 6.6897e-04\n",
            "Epoch 61/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.3736e-04 - val_loss: 1.8469e-04 - learning_rate: 6.6228e-04\n",
            "Epoch 62/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.3282e-04 - val_loss: 2.0825e-04 - learning_rate: 6.5566e-04\n",
            "Epoch 63/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.3822e-04 - val_loss: 1.7346e-04 - learning_rate: 6.4910e-04\n",
            "Epoch 64/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2931e-04 - val_loss: 1.8154e-04 - learning_rate: 6.4261e-04\n",
            "Epoch 65/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2627e-04 - val_loss: 2.2881e-04 - learning_rate: 6.3619e-04\n",
            "Epoch 66/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.3384e-04 - val_loss: 1.7904e-04 - learning_rate: 6.2982e-04\n",
            "Epoch 67/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2677e-04 - val_loss: 1.8096e-04 - learning_rate: 6.2353e-04\n",
            "Epoch 68/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2418e-04 - val_loss: 1.7057e-04 - learning_rate: 6.1729e-04\n",
            "Epoch 69/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2561e-04 - val_loss: 1.6857e-04 - learning_rate: 6.1112e-04\n",
            "Epoch 70/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2481e-04 - val_loss: 1.7629e-04 - learning_rate: 6.0501e-04\n",
            "Epoch 71/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1836e-04 - val_loss: 1.6792e-04 - learning_rate: 5.9896e-04\n",
            "Epoch 72/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2399e-04 - val_loss: 1.6368e-04 - learning_rate: 5.9297e-04\n",
            "Epoch 73/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1656e-04 - val_loss: 1.8333e-04 - learning_rate: 5.8704e-04\n",
            "Epoch 74/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2825e-04 - val_loss: 1.6093e-04 - learning_rate: 5.8117e-04\n",
            "Epoch 75/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1571e-04 - val_loss: 1.5541e-04 - learning_rate: 5.7535e-04\n",
            "Epoch 76/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1315e-04 - val_loss: 1.6607e-04 - learning_rate: 5.6960e-04\n",
            "Epoch 77/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1099e-04 - val_loss: 1.7979e-04 - learning_rate: 5.6391e-04\n",
            "Epoch 78/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1239e-04 - val_loss: 1.6734e-04 - learning_rate: 5.5827e-04\n",
            "Epoch 79/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1327e-04 - val_loss: 1.6467e-04 - learning_rate: 5.5268e-04\n",
            "Epoch 80/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1369e-04 - val_loss: 1.5638e-04 - learning_rate: 5.4716e-04\n",
            "Epoch 81/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0710e-04 - val_loss: 1.6428e-04 - learning_rate: 5.4169e-04\n",
            "Epoch 82/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0661e-04 - val_loss: 1.6485e-04 - learning_rate: 5.3627e-04\n",
            "Epoch 83/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1058e-04 - val_loss: 1.7049e-04 - learning_rate: 5.3091e-04\n",
            "Epoch 84/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0675e-04 - val_loss: 1.5202e-04 - learning_rate: 5.2560e-04\n",
            "Epoch 85/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0469e-04 - val_loss: 1.5262e-04 - learning_rate: 5.2034e-04\n",
            "Epoch 86/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0189e-04 - val_loss: 1.4604e-04 - learning_rate: 5.1514e-04\n",
            "Epoch 87/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0277e-04 - val_loss: 1.6120e-04 - learning_rate: 5.0999e-04\n",
            "Epoch 88/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0660e-04 - val_loss: 1.4555e-04 - learning_rate: 5.0489e-04\n",
            "Epoch 89/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0139e-04 - val_loss: 1.4500e-04 - learning_rate: 4.9984e-04\n",
            "Epoch 90/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.9362e-05 - val_loss: 1.5819e-04 - learning_rate: 4.9484e-04\n",
            "Epoch 91/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0291e-04 - val_loss: 1.4897e-04 - learning_rate: 4.8989e-04\n",
            "Epoch 92/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9.7330e-05 - val_loss: 1.5151e-04 - learning_rate: 4.8499e-04\n",
            "Epoch 93/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9.6730e-05 - val_loss: 1.4053e-04 - learning_rate: 4.8014e-04\n",
            "Epoch 94/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.7898e-05 - val_loss: 1.4904e-04 - learning_rate: 4.7534e-04\n",
            "Epoch 95/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9.9217e-05 - val_loss: 1.5083e-04 - learning_rate: 4.7059e-04\n",
            "Epoch 96/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9.8205e-05 - val_loss: 1.6824e-04 - learning_rate: 4.6588e-04\n",
            "Epoch 97/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9.8282e-05 - val_loss: 1.4406e-04 - learning_rate: 4.6122e-04\n",
            "Epoch 98/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9.5933e-05 - val_loss: 1.3934e-04 - learning_rate: 4.5661e-04\n",
            "Epoch 99/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9.3654e-05 - val_loss: 1.3748e-04 - learning_rate: 4.5204e-04\n",
            "Epoch 100/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9.3638e-05 - val_loss: 1.4043e-04 - learning_rate: 4.4752e-04\n",
            "Epoch 101/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9.2461e-05 - val_loss: 1.3837e-04 - learning_rate: 4.4305e-04\n",
            "Epoch 102/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9.0483e-05 - val_loss: 1.4135e-04 - learning_rate: 4.3862e-04\n",
            "Epoch 103/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9.3006e-05 - val_loss: 1.3911e-04 - learning_rate: 4.3423e-04\n",
            "Epoch 104/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9.2353e-05 - val_loss: 1.4028e-04 - learning_rate: 4.2989e-04\n",
            "Epoch 105/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9.0589e-05 - val_loss: 1.3310e-04 - learning_rate: 4.2559e-04\n",
            "Epoch 106/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9.0017e-05 - val_loss: 1.4143e-04 - learning_rate: 4.2133e-04\n",
            "Epoch 107/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9.0593e-05 - val_loss: 1.3980e-04 - learning_rate: 4.1712e-04\n",
            "Epoch 108/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.9573e-05 - val_loss: 1.3314e-04 - learning_rate: 4.1295e-04\n",
            "Epoch 109/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.8473e-05 - val_loss: 1.3109e-04 - learning_rate: 4.0882e-04\n",
            "Epoch 110/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.5858e-05 - val_loss: 1.3060e-04 - learning_rate: 4.0473e-04\n",
            "Epoch 111/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.5449e-05 - val_loss: 1.3526e-04 - learning_rate: 4.0068e-04\n",
            "Epoch 112/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.6999e-05 - val_loss: 1.3041e-04 - learning_rate: 3.9668e-04\n",
            "Epoch 113/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.7110e-05 - val_loss: 1.2942e-04 - learning_rate: 3.9271e-04\n",
            "Epoch 114/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.7332e-05 - val_loss: 1.3682e-04 - learning_rate: 3.8878e-04\n",
            "Epoch 115/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.5299e-05 - val_loss: 1.2501e-04 - learning_rate: 3.8490e-04\n",
            "Epoch 116/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.3956e-05 - val_loss: 1.3983e-04 - learning_rate: 3.8105e-04\n",
            "Epoch 117/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.4819e-05 - val_loss: 1.3264e-04 - learning_rate: 3.7724e-04\n",
            "Epoch 118/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.3255e-05 - val_loss: 1.2707e-04 - learning_rate: 3.7346e-04\n",
            "Epoch 119/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.6741e-05 - val_loss: 1.2662e-04 - learning_rate: 3.6973e-04\n",
            "Epoch 120/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.1575e-05 - val_loss: 1.2677e-04 - learning_rate: 3.6603e-04\n",
            "Epoch 121/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.2206e-05 - val_loss: 1.3025e-04 - learning_rate: 3.6237e-04\n",
            "Epoch 122/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.2420e-05 - val_loss: 1.3095e-04 - learning_rate: 3.5875e-04\n",
            "Epoch 123/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.2216e-05 - val_loss: 1.2942e-04 - learning_rate: 3.5516e-04\n",
            "Epoch 124/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.0901e-05 - val_loss: 1.2721e-04 - learning_rate: 3.5161e-04\n",
            "Epoch 125/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.9013e-05 - val_loss: 1.2103e-04 - learning_rate: 3.4809e-04\n",
            "Epoch 126/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.7316e-05 - val_loss: 1.2339e-04 - learning_rate: 3.4461e-04\n",
            "Epoch 127/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.0358e-05 - val_loss: 1.2830e-04 - learning_rate: 3.4117e-04\n",
            "Epoch 128/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.0634e-05 - val_loss: 1.4149e-04 - learning_rate: 3.3775e-04\n",
            "Epoch 129/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.0718e-05 - val_loss: 1.3235e-04 - learning_rate: 3.3438e-04\n",
            "Epoch 130/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.0116e-05 - val_loss: 1.2083e-04 - learning_rate: 3.3103e-04\n",
            "Epoch 131/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.6859e-05 - val_loss: 1.3655e-04 - learning_rate: 3.2772e-04\n",
            "Epoch 132/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.9930e-05 - val_loss: 1.2876e-04 - learning_rate: 3.2445e-04\n",
            "Epoch 133/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.9101e-05 - val_loss: 1.2200e-04 - learning_rate: 3.2120e-04\n",
            "Epoch 134/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.7216e-05 - val_loss: 1.3271e-04 - learning_rate: 3.1799e-04\n",
            "Epoch 135/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.8198e-05 - val_loss: 1.2004e-04 - learning_rate: 3.1481e-04\n",
            "Epoch 136/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.6182e-05 - val_loss: 1.2257e-04 - learning_rate: 3.1166e-04\n",
            "Epoch 137/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.6190e-05 - val_loss: 1.1917e-04 - learning_rate: 3.0854e-04\n",
            "Epoch 138/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.6051e-05 - val_loss: 1.2151e-04 - learning_rate: 3.0546e-04\n",
            "Epoch 139/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.5340e-05 - val_loss: 1.2743e-04 - learning_rate: 3.0240e-04\n",
            "Epoch 140/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.5238e-05 - val_loss: 1.1718e-04 - learning_rate: 2.9938e-04\n",
            "Epoch 141/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.4082e-05 - val_loss: 1.2719e-04 - learning_rate: 2.9639e-04\n",
            "Epoch 142/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.4915e-05 - val_loss: 1.2132e-04 - learning_rate: 2.9342e-04\n",
            "Epoch 143/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.6071e-05 - val_loss: 1.1780e-04 - learning_rate: 2.9049e-04\n",
            "Epoch 144/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.3899e-05 - val_loss: 1.1761e-04 - learning_rate: 2.8758e-04\n",
            "Epoch 145/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.2872e-05 - val_loss: 1.2215e-04 - learning_rate: 2.8471e-04\n",
            "Epoch 146/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.3516e-05 - val_loss: 1.2012e-04 - learning_rate: 2.8186e-04\n",
            "Epoch 147/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.3411e-05 - val_loss: 1.1681e-04 - learning_rate: 2.7904e-04\n",
            "Epoch 148/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.2665e-05 - val_loss: 1.2057e-04 - learning_rate: 2.7625e-04\n",
            "Epoch 149/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.2167e-05 - val_loss: 1.1746e-04 - learning_rate: 2.7349e-04\n",
            "Epoch 150/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.2670e-05 - val_loss: 1.3024e-04 - learning_rate: 2.7075e-04\n",
            "Epoch 151/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.5434e-05 - val_loss: 1.2059e-04 - learning_rate: 2.6805e-04\n",
            "Epoch 152/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.2448e-05 - val_loss: 1.1735e-04 - learning_rate: 2.6537e-04\n",
            "Epoch 153/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.0066e-05 - val_loss: 1.2275e-04 - learning_rate: 2.6271e-04\n",
            "Epoch 154/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.1297e-05 - val_loss: 1.1752e-04 - learning_rate: 2.6009e-04\n",
            "Epoch 155/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.0468e-05 - val_loss: 1.1594e-04 - learning_rate: 2.5748e-04\n",
            "Epoch 156/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.1433e-05 - val_loss: 1.2203e-04 - learning_rate: 2.5491e-04\n",
            "Epoch 157/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.1484e-05 - val_loss: 1.1329e-04 - learning_rate: 2.5236e-04\n",
            "Epoch 158/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.9549e-05 - val_loss: 1.1436e-04 - learning_rate: 2.4984e-04\n",
            "Epoch 159/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.9939e-05 - val_loss: 1.2140e-04 - learning_rate: 2.4734e-04\n",
            "Epoch 160/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.9374e-05 - val_loss: 1.1801e-04 - learning_rate: 2.4487e-04\n",
            "Epoch 161/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.0906e-05 - val_loss: 1.2934e-04 - learning_rate: 2.4242e-04\n",
            "Epoch 162/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.1138e-05 - val_loss: 1.1313e-04 - learning_rate: 2.3999e-04\n",
            "Epoch 163/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.7742e-05 - val_loss: 1.1603e-04 - learning_rate: 2.3759e-04\n",
            "Epoch 164/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.8666e-05 - val_loss: 1.1370e-04 - learning_rate: 2.3522e-04\n",
            "Epoch 165/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.9062e-05 - val_loss: 1.1286e-04 - learning_rate: 2.3286e-04\n",
            "Epoch 166/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8265e-05 - val_loss: 1.1220e-04 - learning_rate: 2.3054e-04\n",
            "Epoch 167/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7644e-05 - val_loss: 1.1429e-04 - learning_rate: 2.2823e-04\n",
            "Epoch 168/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.7281e-05 - val_loss: 1.1367e-04 - learning_rate: 2.2595e-04\n",
            "Epoch 169/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.6821e-05 - val_loss: 1.1367e-04 - learning_rate: 2.2369e-04\n",
            "Epoch 170/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.8449e-05 - val_loss: 1.1029e-04 - learning_rate: 2.2145e-04\n",
            "Epoch 171/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7017e-05 - val_loss: 1.1112e-04 - learning_rate: 2.1924e-04\n",
            "Epoch 172/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8776e-05 - val_loss: 1.1560e-04 - learning_rate: 2.1705e-04\n",
            "Epoch 173/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7482e-05 - val_loss: 1.1173e-04 - learning_rate: 2.1487e-04\n",
            "Epoch 174/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.5839e-05 - val_loss: 1.1186e-04 - learning_rate: 2.1273e-04\n",
            "Epoch 175/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.7698e-05 - val_loss: 1.1145e-04 - learning_rate: 2.1060e-04\n",
            "Epoch 176/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.6404e-05 - val_loss: 1.1086e-04 - learning_rate: 2.0849e-04\n",
            "Epoch 177/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6333e-05 - val_loss: 1.1110e-04 - learning_rate: 2.0641e-04\n",
            "Epoch 178/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.5451e-05 - val_loss: 1.1344e-04 - learning_rate: 2.0434e-04\n",
            "Epoch 179/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 6.6346e-05 - val_loss: 1.0944e-04 - learning_rate: 2.0230e-04\n",
            "Epoch 180/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.5498e-05 - val_loss: 1.1100e-04 - learning_rate: 2.0028e-04\n",
            "Epoch 181/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.6093e-05 - val_loss: 1.1184e-04 - learning_rate: 1.9827e-04\n",
            "Epoch 182/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.5548e-05 - val_loss: 1.1232e-04 - learning_rate: 1.9629e-04\n",
            "Epoch 183/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.5254e-05 - val_loss: 1.1149e-04 - learning_rate: 1.9433e-04\n",
            "Epoch 184/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.4352e-05 - val_loss: 1.1163e-04 - learning_rate: 1.9239e-04\n",
            "Epoch 185/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.4881e-05 - val_loss: 1.0851e-04 - learning_rate: 1.9046e-04\n",
            "Epoch 186/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.5421e-05 - val_loss: 1.0964e-04 - learning_rate: 1.8856e-04\n",
            "Epoch 187/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.3942e-05 - val_loss: 1.0939e-04 - learning_rate: 1.8667e-04\n",
            "Epoch 188/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.4439e-05 - val_loss: 1.0883e-04 - learning_rate: 1.8480e-04\n",
            "Epoch 189/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.4082e-05 - val_loss: 1.0932e-04 - learning_rate: 1.8296e-04\n",
            "Epoch 190/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.3862e-05 - val_loss: 1.0918e-04 - learning_rate: 1.8113e-04\n",
            "Epoch 191/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.4035e-05 - val_loss: 1.1119e-04 - learning_rate: 1.7932e-04\n",
            "Epoch 192/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.3570e-05 - val_loss: 1.0777e-04 - learning_rate: 1.7752e-04\n",
            "Epoch 193/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.2429e-05 - val_loss: 1.0809e-04 - learning_rate: 1.7575e-04\n",
            "Epoch 194/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.3823e-05 - val_loss: 1.0829e-04 - learning_rate: 1.7399e-04\n",
            "Epoch 195/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.2629e-05 - val_loss: 1.1105e-04 - learning_rate: 1.7225e-04\n",
            "Epoch 196/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.2543e-05 - val_loss: 1.1128e-04 - learning_rate: 1.7053e-04\n",
            "Epoch 197/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.3325e-05 - val_loss: 1.0592e-04 - learning_rate: 1.6882e-04\n",
            "Epoch 198/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.2941e-05 - val_loss: 1.0628e-04 - learning_rate: 1.6713e-04\n",
            "Epoch 199/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.1756e-05 - val_loss: 1.0922e-04 - learning_rate: 1.6546e-04\n",
            "Epoch 200/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.1475e-05 - val_loss: 1.0534e-04 - learning_rate: 1.6381e-04\n",
            "Epoch 201/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.1609e-05 - val_loss: 1.0675e-04 - learning_rate: 1.6217e-04\n",
            "Epoch 202/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.1877e-05 - val_loss: 1.0719e-04 - learning_rate: 1.6055e-04\n",
            "Epoch 203/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.1869e-05 - val_loss: 1.0660e-04 - learning_rate: 1.5894e-04\n",
            "Epoch 204/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.1051e-05 - val_loss: 1.0547e-04 - learning_rate: 1.5735e-04\n",
            "Epoch 205/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.1697e-05 - val_loss: 1.0828e-04 - learning_rate: 1.5578e-04\n",
            "Epoch 206/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.1921e-05 - val_loss: 1.0713e-04 - learning_rate: 1.5422e-04\n",
            "Epoch 207/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.1447e-05 - val_loss: 1.1203e-04 - learning_rate: 1.5268e-04\n",
            "Epoch 208/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.1042e-05 - val_loss: 1.0524e-04 - learning_rate: 1.5115e-04\n",
            "Epoch 209/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.1284e-05 - val_loss: 1.0756e-04 - learning_rate: 1.4964e-04\n",
            "Epoch 210/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.0604e-05 - val_loss: 1.0565e-04 - learning_rate: 1.4815e-04\n",
            "Epoch 211/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.1127e-05 - val_loss: 1.0725e-04 - learning_rate: 1.4666e-04\n",
            "Epoch 212/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.0672e-05 - val_loss: 1.0639e-04 - learning_rate: 1.4520e-04\n",
            "Epoch 213/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.2049e-05 - val_loss: 1.0780e-04 - learning_rate: 1.4374e-04\n",
            "Epoch 214/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.0686e-05 - val_loss: 1.0642e-04 - learning_rate: 1.4231e-04\n",
            "Epoch 215/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.0768e-05 - val_loss: 1.0481e-04 - learning_rate: 1.4088e-04\n",
            "Epoch 216/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.1248e-05 - val_loss: 1.0609e-04 - learning_rate: 1.3948e-04\n",
            "Epoch 217/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.0007e-05 - val_loss: 1.0551e-04 - learning_rate: 1.3808e-04\n",
            "Epoch 218/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.8934e-05 - val_loss: 1.0350e-04 - learning_rate: 1.3670e-04\n",
            "Epoch 219/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.1124e-05 - val_loss: 1.0529e-04 - learning_rate: 1.3533e-04\n",
            "Epoch 220/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.9468e-05 - val_loss: 1.0442e-04 - learning_rate: 1.3398e-04\n",
            "Epoch 221/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.0166e-05 - val_loss: 1.0837e-04 - learning_rate: 1.3264e-04\n",
            "Epoch 222/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.0218e-05 - val_loss: 1.0479e-04 - learning_rate: 1.3131e-04\n",
            "Epoch 223/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.8869e-05 - val_loss: 1.0412e-04 - learning_rate: 1.3000e-04\n",
            "Epoch 224/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.8883e-05 - val_loss: 1.0344e-04 - learning_rate: 1.2870e-04\n",
            "Epoch 225/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.9407e-05 - val_loss: 1.0792e-04 - learning_rate: 1.2741e-04\n",
            "Epoch 226/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.9533e-05 - val_loss: 1.0822e-04 - learning_rate: 1.2614e-04\n",
            "Epoch 227/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.9142e-05 - val_loss: 1.0401e-04 - learning_rate: 1.2488e-04\n",
            "Epoch 228/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.9195e-05 - val_loss: 1.0821e-04 - learning_rate: 1.2363e-04\n",
            "Epoch 229/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.9316e-05 - val_loss: 1.0356e-04 - learning_rate: 1.2239e-04\n",
            "Epoch 230/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.8892e-05 - val_loss: 1.0459e-04 - learning_rate: 1.2117e-04\n",
            "Epoch 231/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.8235e-05 - val_loss: 1.0406e-04 - learning_rate: 1.1996e-04\n",
            "Epoch 232/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.8492e-05 - val_loss: 1.0261e-04 - learning_rate: 1.1876e-04\n",
            "Epoch 233/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.7511e-05 - val_loss: 1.0225e-04 - learning_rate: 1.1757e-04\n",
            "Epoch 234/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.8221e-05 - val_loss: 1.0463e-04 - learning_rate: 1.1639e-04\n",
            "Epoch 235/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.8122e-05 - val_loss: 1.0358e-04 - learning_rate: 1.1523e-04\n",
            "Epoch 236/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.8624e-05 - val_loss: 1.0361e-04 - learning_rate: 1.1408e-04\n",
            "Epoch 237/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.7824e-05 - val_loss: 1.0362e-04 - learning_rate: 1.1294e-04\n",
            "Epoch 238/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.8939e-05 - val_loss: 1.0166e-04 - learning_rate: 1.1181e-04\n",
            "Epoch 239/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.8108e-05 - val_loss: 1.0150e-04 - learning_rate: 1.1069e-04\n",
            "Epoch 240/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.7597e-05 - val_loss: 1.0421e-04 - learning_rate: 1.0958e-04\n",
            "Epoch 241/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.8189e-05 - val_loss: 1.0550e-04 - learning_rate: 1.0849e-04\n",
            "Epoch 242/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.7959e-05 - val_loss: 1.0445e-04 - learning_rate: 1.0740e-04\n",
            "Epoch 243/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.7446e-05 - val_loss: 1.0091e-04 - learning_rate: 1.0633e-04\n",
            "Epoch 244/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.7816e-05 - val_loss: 1.0138e-04 - learning_rate: 1.0526e-04\n",
            "Epoch 245/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.6984e-05 - val_loss: 1.0280e-04 - learning_rate: 1.0421e-04\n",
            "Epoch 246/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.7536e-05 - val_loss: 1.0247e-04 - learning_rate: 1.0317e-04\n",
            "Epoch 247/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.6929e-05 - val_loss: 1.0313e-04 - learning_rate: 1.0214e-04\n",
            "Epoch 248/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.6401e-05 - val_loss: 1.0195e-04 - learning_rate: 1.0112e-04\n",
            "Epoch 249/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.6799e-05 - val_loss: 1.0182e-04 - learning_rate: 1.0011e-04\n",
            "Epoch 250/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.6406e-05 - val_loss: 1.0159e-04 - learning_rate: 9.9105e-05\n",
            "Epoch 251/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.6153e-05 - val_loss: 1.0272e-04 - learning_rate: 9.8114e-05\n",
            "Epoch 252/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.6850e-05 - val_loss: 1.0139e-04 - learning_rate: 9.7133e-05\n",
            "Epoch 253/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.6205e-05 - val_loss: 1.0150e-04 - learning_rate: 9.6161e-05\n",
            "Epoch 254/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.6384e-05 - val_loss: 1.0066e-04 - learning_rate: 9.5200e-05\n",
            "Epoch 255/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.6467e-05 - val_loss: 1.0276e-04 - learning_rate: 9.4248e-05\n",
            "Epoch 256/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.6990e-05 - val_loss: 1.0321e-04 - learning_rate: 9.3305e-05\n",
            "Epoch 257/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.6516e-05 - val_loss: 1.0047e-04 - learning_rate: 9.2372e-05\n",
            "Epoch 258/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.6521e-05 - val_loss: 1.0058e-04 - learning_rate: 9.1448e-05\n",
            "Epoch 259/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.6503e-05 - val_loss: 1.0138e-04 - learning_rate: 9.0534e-05\n",
            "Epoch 260/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.6499e-05 - val_loss: 1.0084e-04 - learning_rate: 8.9629e-05\n",
            "Epoch 261/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.5843e-05 - val_loss: 1.0175e-04 - learning_rate: 8.8732e-05\n",
            "Epoch 262/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.5650e-05 - val_loss: 9.9959e-05 - learning_rate: 8.7845e-05\n",
            "Epoch 263/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.5559e-05 - val_loss: 1.0204e-04 - learning_rate: 8.6967e-05\n",
            "Epoch 264/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.6238e-05 - val_loss: 1.0089e-04 - learning_rate: 8.6097e-05\n",
            "Epoch 265/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.5381e-05 - val_loss: 9.9929e-05 - learning_rate: 8.5236e-05\n",
            "Epoch 266/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.5430e-05 - val_loss: 9.9610e-05 - learning_rate: 8.4384e-05\n",
            "Epoch 267/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.5512e-05 - val_loss: 1.0072e-04 - learning_rate: 8.3540e-05\n",
            "Epoch 268/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.5320e-05 - val_loss: 1.0095e-04 - learning_rate: 8.2704e-05\n",
            "Epoch 269/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.5033e-05 - val_loss: 9.8905e-05 - learning_rate: 8.1877e-05\n",
            "Epoch 270/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.5583e-05 - val_loss: 1.0151e-04 - learning_rate: 8.1059e-05\n",
            "Epoch 271/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.5098e-05 - val_loss: 9.9742e-05 - learning_rate: 8.0248e-05\n",
            "Epoch 272/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.4971e-05 - val_loss: 1.0087e-04 - learning_rate: 7.9445e-05\n",
            "Epoch 273/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.5619e-05 - val_loss: 9.9482e-05 - learning_rate: 7.8651e-05\n",
            "Epoch 274/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.4521e-05 - val_loss: 1.0019e-04 - learning_rate: 7.7865e-05\n",
            "Epoch 275/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.4738e-05 - val_loss: 9.9648e-05 - learning_rate: 7.7086e-05\n",
            "Epoch 276/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.4949e-05 - val_loss: 9.9092e-05 - learning_rate: 7.6315e-05\n",
            "Epoch 277/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.5052e-05 - val_loss: 1.0012e-04 - learning_rate: 7.5552e-05\n",
            "Epoch 278/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.5090e-05 - val_loss: 9.9405e-05 - learning_rate: 7.4796e-05\n",
            "Epoch 279/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.4682e-05 - val_loss: 9.8852e-05 - learning_rate: 7.4048e-05\n",
            "Epoch 280/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.4665e-05 - val_loss: 9.8845e-05 - learning_rate: 7.3308e-05\n",
            "Epoch 281/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.4569e-05 - val_loss: 9.9720e-05 - learning_rate: 7.2575e-05\n",
            "Epoch 282/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.4552e-05 - val_loss: 9.9530e-05 - learning_rate: 7.1849e-05\n",
            "Epoch 283/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.5088e-05 - val_loss: 9.8822e-05 - learning_rate: 7.1131e-05\n",
            "Epoch 284/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.4016e-05 - val_loss: 9.8020e-05 - learning_rate: 7.0419e-05\n",
            "Epoch 285/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.3977e-05 - val_loss: 9.9719e-05 - learning_rate: 6.9715e-05\n",
            "Epoch 286/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.4228e-05 - val_loss: 9.8738e-05 - learning_rate: 6.9018e-05\n",
            "Epoch 287/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.4201e-05 - val_loss: 1.0002e-04 - learning_rate: 6.8328e-05\n",
            "Epoch 288/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.4495e-05 - val_loss: 9.8531e-05 - learning_rate: 6.7644e-05\n",
            "Epoch 289/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.3826e-05 - val_loss: 9.9134e-05 - learning_rate: 6.6968e-05\n",
            "Epoch 290/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.3888e-05 - val_loss: 9.8361e-05 - learning_rate: 6.6298e-05\n",
            "Epoch 291/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.3645e-05 - val_loss: 9.9584e-05 - learning_rate: 6.5635e-05\n",
            "Epoch 292/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.3601e-05 - val_loss: 9.9004e-05 - learning_rate: 6.4979e-05\n",
            "Epoch 293/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.3558e-05 - val_loss: 9.8236e-05 - learning_rate: 6.4329e-05\n",
            "Epoch 294/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.3407e-05 - val_loss: 1.0132e-04 - learning_rate: 6.3686e-05\n",
            "Epoch 295/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.4712e-05 - val_loss: 9.8111e-05 - learning_rate: 6.3049e-05\n",
            "Epoch 296/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.3530e-05 - val_loss: 9.7856e-05 - learning_rate: 6.2419e-05\n",
            "Epoch 297/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.3824e-05 - val_loss: 9.9069e-05 - learning_rate: 6.1794e-05\n",
            "Epoch 298/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.3882e-05 - val_loss: 9.8880e-05 - learning_rate: 6.1176e-05\n",
            "Epoch 299/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.3327e-05 - val_loss: 9.8307e-05 - learning_rate: 6.0565e-05\n",
            "Epoch 300/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.3376e-05 - val_loss: 9.8687e-05 - learning_rate: 5.9959e-05\n",
            "Epoch 301/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.3093e-05 - val_loss: 9.7797e-05 - learning_rate: 5.9359e-05\n",
            "Epoch 302/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.3246e-05 - val_loss: 9.8172e-05 - learning_rate: 5.8766e-05\n",
            "Epoch 303/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.3338e-05 - val_loss: 9.8474e-05 - learning_rate: 5.8178e-05\n",
            "Epoch 304/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.3519e-05 - val_loss: 9.8916e-05 - learning_rate: 5.7596e-05\n",
            "Epoch 305/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.4102e-05 - val_loss: 9.7730e-05 - learning_rate: 5.7020e-05\n",
            "Epoch 306/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.3533e-05 - val_loss: 9.8948e-05 - learning_rate: 5.6450e-05\n",
            "Epoch 307/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.3797e-05 - val_loss: 9.7341e-05 - learning_rate: 5.5886e-05\n",
            "Epoch 308/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.3261e-05 - val_loss: 9.9700e-05 - learning_rate: 5.5327e-05\n",
            "Epoch 309/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.3888e-05 - val_loss: 9.8404e-05 - learning_rate: 5.4774e-05\n",
            "Epoch 310/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.3041e-05 - val_loss: 9.7649e-05 - learning_rate: 5.4226e-05\n",
            "Epoch 311/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.3042e-05 - val_loss: 9.7300e-05 - learning_rate: 5.3684e-05\n",
            "Epoch 312/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.3572e-05 - val_loss: 9.8167e-05 - learning_rate: 5.3147e-05\n",
            "Epoch 313/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.3332e-05 - val_loss: 9.7068e-05 - learning_rate: 5.2615e-05\n",
            "Epoch 314/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.2178e-05 - val_loss: 9.7404e-05 - learning_rate: 5.2089e-05\n",
            "Epoch 315/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.2476e-05 - val_loss: 9.7268e-05 - learning_rate: 5.1568e-05\n",
            "Epoch 316/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.2843e-05 - val_loss: 9.7806e-05 - learning_rate: 5.1053e-05\n",
            "Epoch 317/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.2691e-05 - val_loss: 9.7822e-05 - learning_rate: 5.0542e-05\n",
            "Epoch 318/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.2836e-05 - val_loss: 9.7655e-05 - learning_rate: 5.0037e-05\n",
            "Epoch 319/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.3139e-05 - val_loss: 9.7842e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 320/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.2739e-05 - val_loss: 9.9084e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 321/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.2814e-05 - val_loss: 9.7222e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 322/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.2918e-05 - val_loss: 9.9019e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 323/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.2412e-05 - val_loss: 9.7734e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 324/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.3196e-05 - val_loss: 9.7366e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 325/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.2781e-05 - val_loss: 9.9206e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 326/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.2310e-05 - val_loss: 9.7507e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 327/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.3188e-05 - val_loss: 9.7227e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 328/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.2511e-05 - val_loss: 9.6711e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 329/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.2230e-05 - val_loss: 9.7235e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 330/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.2478e-05 - val_loss: 9.7209e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 331/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.2134e-05 - val_loss: 9.7208e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 332/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.2939e-05 - val_loss: 9.7486e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 333/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.2700e-05 - val_loss: 9.7502e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 334/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.2705e-05 - val_loss: 9.7293e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 335/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.2423e-05 - val_loss: 9.8077e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 336/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.2465e-05 - val_loss: 9.7489e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 337/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.3075e-05 - val_loss: 9.7617e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 338/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.1552e-05 - val_loss: 9.6772e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 339/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.2343e-05 - val_loss: 9.6958e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 340/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.2081e-05 - val_loss: 9.7069e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 341/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1634e-05 - val_loss: 9.6687e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 342/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.2412e-05 - val_loss: 9.6639e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 343/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1575e-05 - val_loss: 9.6715e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 344/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.2448e-05 - val_loss: 9.7748e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 345/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.2168e-05 - val_loss: 9.6277e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 346/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1784e-05 - val_loss: 9.7315e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 347/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.2380e-05 - val_loss: 9.7175e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 348/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1915e-05 - val_loss: 9.7022e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 349/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.2096e-05 - val_loss: 9.6026e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 350/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.2383e-05 - val_loss: 9.6482e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 351/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1911e-05 - val_loss: 9.7043e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 352/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1823e-05 - val_loss: 9.8203e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 353/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.2330e-05 - val_loss: 9.8018e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 354/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1928e-05 - val_loss: 9.7137e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 355/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.2159e-05 - val_loss: 9.6737e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 356/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1681e-05 - val_loss: 9.7605e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 357/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1733e-05 - val_loss: 9.7206e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 358/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1591e-05 - val_loss: 9.6670e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 359/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.1606e-05 - val_loss: 9.7397e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 360/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1704e-05 - val_loss: 9.6441e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 361/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.2056e-05 - val_loss: 9.6108e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 362/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1459e-05 - val_loss: 9.6932e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 363/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1340e-05 - val_loss: 9.6713e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 364/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.1282e-05 - val_loss: 9.8681e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 365/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1886e-05 - val_loss: 9.7093e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 366/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1975e-05 - val_loss: 9.9160e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 367/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1824e-05 - val_loss: 9.7078e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 368/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.1758e-05 - val_loss: 9.6867e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 369/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1340e-05 - val_loss: 9.5917e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 370/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1720e-05 - val_loss: 9.6582e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 371/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1679e-05 - val_loss: 9.5776e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 372/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.1300e-05 - val_loss: 9.6485e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 373/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.2054e-05 - val_loss: 9.5671e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 374/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1612e-05 - val_loss: 9.6059e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 375/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1138e-05 - val_loss: 9.6729e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 376/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1398e-05 - val_loss: 9.5599e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 377/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.1011e-05 - val_loss: 9.6548e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 378/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1294e-05 - val_loss: 9.6841e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 379/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1954e-05 - val_loss: 9.6366e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 380/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1227e-05 - val_loss: 9.5740e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 381/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1510e-05 - val_loss: 9.6149e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 382/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.0719e-05 - val_loss: 9.5721e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 383/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1201e-05 - val_loss: 9.5197e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 384/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1262e-05 - val_loss: 9.5868e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 385/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1416e-05 - val_loss: 9.5629e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 386/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.1049e-05 - val_loss: 9.7259e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 387/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1187e-05 - val_loss: 9.6269e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 388/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1362e-05 - val_loss: 9.6489e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 389/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1296e-05 - val_loss: 9.6140e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 390/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.0839e-05 - val_loss: 9.5245e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 391/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1097e-05 - val_loss: 9.5798e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 392/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1428e-05 - val_loss: 9.5727e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 393/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.0709e-05 - val_loss: 9.5914e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 394/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.0758e-05 - val_loss: 9.6420e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 395/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0992e-05 - val_loss: 9.5124e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 396/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1189e-05 - val_loss: 9.6176e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 397/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1054e-05 - val_loss: 9.5938e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 398/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1416e-05 - val_loss: 9.4802e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 399/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.0819e-05 - val_loss: 9.6901e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 400/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1031e-05 - val_loss: 9.6098e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 401/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1211e-05 - val_loss: 9.5818e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 402/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.0823e-05 - val_loss: 9.7057e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 403/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1174e-05 - val_loss: 9.5476e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 404/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0959e-05 - val_loss: 9.5233e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 405/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1062e-05 - val_loss: 9.5394e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 406/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.0849e-05 - val_loss: 9.5834e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 407/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.0672e-05 - val_loss: 9.6097e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 408/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1057e-05 - val_loss: 9.6105e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 409/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.0420e-05 - val_loss: 9.5210e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 410/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.0491e-05 - val_loss: 9.5338e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 411/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.0775e-05 - val_loss: 9.5573e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 412/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0792e-05 - val_loss: 9.5687e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 413/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0556e-05 - val_loss: 9.5025e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 414/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.0836e-05 - val_loss: 9.5658e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 415/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.0456e-05 - val_loss: 9.4660e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 416/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.0214e-05 - val_loss: 9.5873e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 417/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.0608e-05 - val_loss: 9.8677e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 418/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.0850e-05 - val_loss: 9.5793e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 419/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.0419e-05 - val_loss: 9.5480e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 420/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.0530e-05 - val_loss: 9.5655e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 421/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0216e-05 - val_loss: 9.5747e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 422/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1036e-05 - val_loss: 9.4465e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 423/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9964e-05 - val_loss: 9.5556e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 424/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.0783e-05 - val_loss: 9.5723e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 425/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0359e-05 - val_loss: 9.5000e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 426/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0915e-05 - val_loss: 9.4596e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 427/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0397e-05 - val_loss: 9.4699e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 428/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 5.0068e-05 - val_loss: 9.6065e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 429/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9979e-05 - val_loss: 9.4944e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 430/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.0264e-05 - val_loss: 9.4193e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 431/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9242e-05 - val_loss: 9.5734e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 432/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0442e-05 - val_loss: 9.4717e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 433/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9723e-05 - val_loss: 9.6235e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 434/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0538e-05 - val_loss: 9.5854e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 435/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.0083e-05 - val_loss: 9.4086e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 436/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.0146e-05 - val_loss: 9.4842e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 437/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9832e-05 - val_loss: 9.4089e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 438/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0539e-05 - val_loss: 9.4289e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 439/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.0454e-05 - val_loss: 9.4727e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 440/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9951e-05 - val_loss: 9.6240e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 441/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9895e-05 - val_loss: 9.4773e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 442/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.0019e-05 - val_loss: 9.4192e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 443/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.0148e-05 - val_loss: 9.5058e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 444/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.0277e-05 - val_loss: 9.4199e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 445/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9743e-05 - val_loss: 9.3940e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 446/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.0186e-05 - val_loss: 9.4696e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 447/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0208e-05 - val_loss: 9.4051e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 448/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9876e-05 - val_loss: 9.4986e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 449/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9888e-05 - val_loss: 9.3699e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 450/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0550e-05 - val_loss: 9.3704e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 451/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9955e-05 - val_loss: 9.4894e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 452/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9887e-05 - val_loss: 9.3910e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 453/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9793e-05 - val_loss: 9.4585e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 454/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9697e-05 - val_loss: 9.3816e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 455/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9439e-05 - val_loss: 9.4738e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 456/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0250e-05 - val_loss: 9.3982e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 457/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9611e-05 - val_loss: 9.4841e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 458/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9334e-05 - val_loss: 9.4406e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 459/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9964e-05 - val_loss: 9.5040e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 460/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9746e-05 - val_loss: 9.4616e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 461/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9725e-05 - val_loss: 9.4122e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 462/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8993e-05 - val_loss: 9.3943e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 463/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.0071e-05 - val_loss: 9.4018e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 464/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9759e-05 - val_loss: 9.4653e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 465/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9894e-05 - val_loss: 9.4983e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 466/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9561e-05 - val_loss: 9.4420e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 467/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9025e-05 - val_loss: 9.4334e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 468/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9730e-05 - val_loss: 9.3550e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 469/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9598e-05 - val_loss: 9.4857e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 470/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9359e-05 - val_loss: 9.3911e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 471/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9559e-05 - val_loss: 9.5069e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 472/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.9748e-05 - val_loss: 9.3979e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 473/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9361e-05 - val_loss: 9.3558e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 474/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9201e-05 - val_loss: 9.4199e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 475/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9299e-05 - val_loss: 9.4968e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 476/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.0117e-05 - val_loss: 9.3928e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 477/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9217e-05 - val_loss: 9.3492e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 478/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9503e-05 - val_loss: 9.3520e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 479/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9531e-05 - val_loss: 9.4481e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 480/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9329e-05 - val_loss: 9.4226e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 481/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9232e-05 - val_loss: 9.4306e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 482/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9134e-05 - val_loss: 9.3569e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 483/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9114e-05 - val_loss: 9.4770e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 484/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9070e-05 - val_loss: 9.4646e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 485/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9132e-05 - val_loss: 9.3844e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 486/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9308e-05 - val_loss: 9.4113e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 487/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.9119e-05 - val_loss: 9.4427e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 488/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9092e-05 - val_loss: 9.4197e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 489/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9525e-05 - val_loss: 9.3213e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 490/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9241e-05 - val_loss: 9.3114e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 491/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8533e-05 - val_loss: 9.4737e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 492/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9391e-05 - val_loss: 9.3775e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 493/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9296e-05 - val_loss: 9.4022e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 494/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9133e-05 - val_loss: 9.2972e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 495/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8906e-05 - val_loss: 9.3172e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 496/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9097e-05 - val_loss: 9.2708e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 497/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8903e-05 - val_loss: 9.4786e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 498/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9008e-05 - val_loss: 9.3576e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 499/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8523e-05 - val_loss: 9.3486e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 500/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8732e-05 - val_loss: 9.3301e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 501/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8727e-05 - val_loss: 9.3244e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 502/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9007e-05 - val_loss: 9.3456e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 503/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8862e-05 - val_loss: 9.4121e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 504/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8740e-05 - val_loss: 9.3384e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 505/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8775e-05 - val_loss: 9.3504e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 506/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8531e-05 - val_loss: 9.4364e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 507/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.8635e-05 - val_loss: 9.4084e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 508/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8725e-05 - val_loss: 9.3106e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 509/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8684e-05 - val_loss: 9.5097e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 510/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9283e-05 - val_loss: 9.2647e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 511/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9108e-05 - val_loss: 9.3333e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 512/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8333e-05 - val_loss: 9.3319e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 513/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8267e-05 - val_loss: 9.2884e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 514/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8890e-05 - val_loss: 9.3816e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 515/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8385e-05 - val_loss: 9.3849e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 516/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8828e-05 - val_loss: 9.2846e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 517/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8622e-05 - val_loss: 9.2896e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 518/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8540e-05 - val_loss: 9.3030e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 519/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8632e-05 - val_loss: 9.2848e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 520/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8574e-05 - val_loss: 9.3095e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 521/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8729e-05 - val_loss: 9.2628e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 522/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7968e-05 - val_loss: 9.2551e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 523/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8661e-05 - val_loss: 9.2920e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 524/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8374e-05 - val_loss: 9.3752e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 525/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7805e-05 - val_loss: 9.2392e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 526/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8336e-05 - val_loss: 9.2347e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 527/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8118e-05 - val_loss: 9.2944e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 528/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8641e-05 - val_loss: 9.2800e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 529/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8452e-05 - val_loss: 9.3442e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 530/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8735e-05 - val_loss: 9.3777e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 531/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8032e-05 - val_loss: 9.3109e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 532/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8422e-05 - val_loss: 9.2764e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 533/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8665e-05 - val_loss: 9.2769e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 534/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8519e-05 - val_loss: 9.2747e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 535/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8544e-05 - val_loss: 9.2681e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 536/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8287e-05 - val_loss: 9.3250e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 537/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8382e-05 - val_loss: 9.2278e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 538/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7644e-05 - val_loss: 9.3796e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 539/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8136e-05 - val_loss: 9.2746e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 540/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8057e-05 - val_loss: 9.3168e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 541/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8145e-05 - val_loss: 9.2637e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 542/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.7730e-05 - val_loss: 9.2926e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 543/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8285e-05 - val_loss: 9.3641e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 544/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7722e-05 - val_loss: 9.3190e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 545/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8339e-05 - val_loss: 9.3880e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 546/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8008e-05 - val_loss: 9.2509e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 547/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7935e-05 - val_loss: 9.2508e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 548/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.8237e-05 - val_loss: 9.2711e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 549/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.7971e-05 - val_loss: 9.2313e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 550/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.7808e-05 - val_loss: 9.2981e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 551/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.8367e-05 - val_loss: 9.2597e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 552/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7438e-05 - val_loss: 9.2674e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 553/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7665e-05 - val_loss: 9.2725e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 554/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7898e-05 - val_loss: 9.3758e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 555/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8512e-05 - val_loss: 9.1785e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 556/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8319e-05 - val_loss: 9.3307e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 557/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.8044e-05 - val_loss: 9.1945e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 558/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7775e-05 - val_loss: 9.2243e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 559/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8343e-05 - val_loss: 9.1941e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 560/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7744e-05 - val_loss: 9.2361e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 561/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7587e-05 - val_loss: 9.1914e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 562/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8420e-05 - val_loss: 9.2916e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 563/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7704e-05 - val_loss: 9.2707e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 564/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7794e-05 - val_loss: 9.2124e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 565/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8048e-05 - val_loss: 9.2778e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 566/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8548e-05 - val_loss: 9.2489e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 567/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7915e-05 - val_loss: 9.1997e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 568/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8020e-05 - val_loss: 9.2085e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 569/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7392e-05 - val_loss: 9.2215e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 570/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7387e-05 - val_loss: 9.2812e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 571/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7741e-05 - val_loss: 9.2858e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 572/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7736e-05 - val_loss: 9.3485e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 573/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7305e-05 - val_loss: 9.2570e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 574/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7765e-05 - val_loss: 9.1838e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 575/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7611e-05 - val_loss: 9.2152e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 576/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7739e-05 - val_loss: 9.3209e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 577/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.8081e-05 - val_loss: 9.2549e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 578/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7335e-05 - val_loss: 9.1955e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 579/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7550e-05 - val_loss: 9.1949e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 580/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7472e-05 - val_loss: 9.1823e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 581/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7566e-05 - val_loss: 9.2899e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 582/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7588e-05 - val_loss: 9.2035e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 583/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7951e-05 - val_loss: 9.2527e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 584/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7067e-05 - val_loss: 9.3244e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 585/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7337e-05 - val_loss: 9.1962e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 586/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7590e-05 - val_loss: 9.2665e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 587/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7484e-05 - val_loss: 9.2646e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 588/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7878e-05 - val_loss: 9.2412e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 589/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7705e-05 - val_loss: 9.2534e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 590/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7254e-05 - val_loss: 9.1649e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 591/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6973e-05 - val_loss: 9.2329e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 592/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7132e-05 - val_loss: 9.1680e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 593/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7725e-05 - val_loss: 9.1723e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 594/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7735e-05 - val_loss: 9.2441e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 595/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7524e-05 - val_loss: 9.1751e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 596/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7385e-05 - val_loss: 9.2604e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 597/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7683e-05 - val_loss: 9.1691e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 598/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7176e-05 - val_loss: 9.1580e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 599/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7500e-05 - val_loss: 9.1513e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 600/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7231e-05 - val_loss: 9.1568e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 601/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6901e-05 - val_loss: 9.1978e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 602/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.7392e-05 - val_loss: 9.0850e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 603/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6946e-05 - val_loss: 9.1476e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 604/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7153e-05 - val_loss: 9.4520e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 605/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7075e-05 - val_loss: 9.1377e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 606/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7170e-05 - val_loss: 9.1739e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 607/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7037e-05 - val_loss: 9.1902e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 608/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7176e-05 - val_loss: 9.1471e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 609/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7293e-05 - val_loss: 9.1006e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 610/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6954e-05 - val_loss: 9.3837e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 611/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7416e-05 - val_loss: 9.1677e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 612/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6655e-05 - val_loss: 9.1127e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 613/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7040e-05 - val_loss: 9.0916e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 614/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7036e-05 - val_loss: 9.2337e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 615/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6865e-05 - val_loss: 9.1738e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 616/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6768e-05 - val_loss: 9.1371e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 617/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7207e-05 - val_loss: 9.1102e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 618/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6648e-05 - val_loss: 9.2530e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 619/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7463e-05 - val_loss: 9.1004e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 620/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6814e-05 - val_loss: 9.1137e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 621/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6873e-05 - val_loss: 9.1253e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 622/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7095e-05 - val_loss: 9.0818e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 623/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7177e-05 - val_loss: 9.0908e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 624/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6575e-05 - val_loss: 9.0447e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 625/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7108e-05 - val_loss: 9.2587e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 626/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7499e-05 - val_loss: 9.0681e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 627/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6752e-05 - val_loss: 9.1329e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 628/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6590e-05 - val_loss: 9.0986e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 629/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6849e-05 - val_loss: 9.1103e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 630/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6829e-05 - val_loss: 9.0536e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 631/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6422e-05 - val_loss: 9.1636e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 632/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6492e-05 - val_loss: 9.1047e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 633/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6550e-05 - val_loss: 9.1054e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 634/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6550e-05 - val_loss: 9.0853e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 635/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6499e-05 - val_loss: 9.1670e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 636/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6496e-05 - val_loss: 9.0239e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 637/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6187e-05 - val_loss: 9.1481e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 638/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6896e-05 - val_loss: 9.0740e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 639/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6982e-05 - val_loss: 9.1254e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 640/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6406e-05 - val_loss: 9.1504e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 641/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5833e-05 - val_loss: 9.0909e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 642/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.6767e-05 - val_loss: 9.1885e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 643/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6911e-05 - val_loss: 9.2144e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 644/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6615e-05 - val_loss: 9.0396e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 645/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6086e-05 - val_loss: 9.0962e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 646/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6376e-05 - val_loss: 9.0274e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 647/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6439e-05 - val_loss: 9.1936e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 648/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6167e-05 - val_loss: 9.1792e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 649/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6427e-05 - val_loss: 9.1188e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 650/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6384e-05 - val_loss: 9.0906e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 651/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6790e-05 - val_loss: 9.0932e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 652/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6289e-05 - val_loss: 9.0169e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 653/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6524e-05 - val_loss: 9.0649e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 654/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6511e-05 - val_loss: 9.0762e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 655/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6521e-05 - val_loss: 9.2103e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 656/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6263e-05 - val_loss: 9.0470e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 657/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5983e-05 - val_loss: 9.0733e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 658/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6289e-05 - val_loss: 9.0716e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 659/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.6237e-05 - val_loss: 9.0060e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 660/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5847e-05 - val_loss: 9.0243e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 661/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6083e-05 - val_loss: 9.0273e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 662/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6235e-05 - val_loss: 9.0141e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 663/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6175e-05 - val_loss: 9.0789e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 664/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6184e-05 - val_loss: 9.0719e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 665/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.6256e-05 - val_loss: 9.0978e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 666/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6271e-05 - val_loss: 9.2773e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 667/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5931e-05 - val_loss: 9.0256e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 668/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6215e-05 - val_loss: 9.1138e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 669/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5983e-05 - val_loss: 9.1127e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 670/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6167e-05 - val_loss: 9.1801e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 671/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5893e-05 - val_loss: 9.0421e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 672/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5663e-05 - val_loss: 9.0796e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 673/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6097e-05 - val_loss: 9.1997e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 674/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5841e-05 - val_loss: 9.0792e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 675/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6615e-05 - val_loss: 9.0489e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 676/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6452e-05 - val_loss: 9.0517e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 677/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5739e-05 - val_loss: 9.0794e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 678/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6211e-05 - val_loss: 9.1873e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 679/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6130e-05 - val_loss: 9.1096e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 680/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5886e-05 - val_loss: 8.9955e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 681/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5980e-05 - val_loss: 9.1800e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 682/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6036e-05 - val_loss: 9.0108e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 683/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5773e-05 - val_loss: 9.0432e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 684/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5960e-05 - val_loss: 9.0723e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 685/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5621e-05 - val_loss: 8.9980e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 686/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6038e-05 - val_loss: 8.9562e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 687/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.6035e-05 - val_loss: 8.9630e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 688/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5933e-05 - val_loss: 9.0981e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 689/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6118e-05 - val_loss: 9.0559e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 690/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6193e-05 - val_loss: 9.0170e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 691/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5597e-05 - val_loss: 9.0219e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 692/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5542e-05 - val_loss: 9.0059e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 693/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5778e-05 - val_loss: 9.1064e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 694/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5874e-05 - val_loss: 9.2558e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 695/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5707e-05 - val_loss: 8.9957e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 696/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5625e-05 - val_loss: 9.0181e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 697/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5803e-05 - val_loss: 9.0394e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 698/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5814e-05 - val_loss: 8.9900e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 699/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5822e-05 - val_loss: 8.9613e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 700/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5694e-05 - val_loss: 9.0142e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 701/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6087e-05 - val_loss: 9.0319e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 702/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5457e-05 - val_loss: 9.0288e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 703/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.5384e-05 - val_loss: 8.9519e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 704/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5478e-05 - val_loss: 9.1163e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 705/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5539e-05 - val_loss: 8.9650e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 706/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5537e-05 - val_loss: 8.9542e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 707/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5756e-05 - val_loss: 8.9325e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 708/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5776e-05 - val_loss: 8.9305e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 709/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5519e-05 - val_loss: 8.9743e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 710/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5467e-05 - val_loss: 9.0120e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 711/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5428e-05 - val_loss: 9.0457e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 712/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5481e-05 - val_loss: 8.9964e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 713/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5194e-05 - val_loss: 9.0171e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 714/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5457e-05 - val_loss: 9.0367e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 715/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5162e-05 - val_loss: 9.1829e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 716/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5603e-05 - val_loss: 9.0230e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 717/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5510e-05 - val_loss: 8.9874e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 718/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5419e-05 - val_loss: 9.0451e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 719/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5150e-05 - val_loss: 8.9466e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 720/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5747e-05 - val_loss: 9.0317e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 721/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5496e-05 - val_loss: 8.9613e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 722/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5144e-05 - val_loss: 8.9830e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 723/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.5433e-05 - val_loss: 9.2590e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 724/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5953e-05 - val_loss: 9.1212e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 725/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5090e-05 - val_loss: 9.0827e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 726/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5284e-05 - val_loss: 8.9706e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 727/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5270e-05 - val_loss: 8.9508e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 728/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5454e-05 - val_loss: 8.9269e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 729/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5034e-05 - val_loss: 8.9698e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 730/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5011e-05 - val_loss: 8.9839e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 731/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5040e-05 - val_loss: 8.9237e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 732/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4706e-05 - val_loss: 9.1238e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 733/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.5286e-05 - val_loss: 8.9243e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 734/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5377e-05 - val_loss: 9.0121e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 735/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5603e-05 - val_loss: 9.0241e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 736/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5147e-05 - val_loss: 9.0002e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 737/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4853e-05 - val_loss: 9.0249e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 738/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.5289e-05 - val_loss: 9.0322e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 739/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5004e-05 - val_loss: 8.9129e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 740/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5363e-05 - val_loss: 8.9272e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 741/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5274e-05 - val_loss: 8.9418e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 742/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5322e-05 - val_loss: 9.0287e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 743/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5084e-05 - val_loss: 8.9682e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 744/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5219e-05 - val_loss: 8.9218e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 745/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4883e-05 - val_loss: 8.9056e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 746/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4880e-05 - val_loss: 8.9431e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 747/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5336e-05 - val_loss: 8.9811e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 748/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.4496e-05 - val_loss: 8.9282e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 749/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4922e-05 - val_loss: 8.9037e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 750/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4821e-05 - val_loss: 8.9216e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 751/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4835e-05 - val_loss: 8.9325e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 752/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4818e-05 - val_loss: 8.8740e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 753/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4414e-05 - val_loss: 8.8695e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 754/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4172e-05 - val_loss: 8.8629e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 755/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4512e-05 - val_loss: 9.0774e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 756/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5055e-05 - val_loss: 8.8773e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 757/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4722e-05 - val_loss: 8.8790e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 758/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4570e-05 - val_loss: 8.9791e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 759/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5211e-05 - val_loss: 8.9077e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 760/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4306e-05 - val_loss: 8.8599e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 761/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4417e-05 - val_loss: 9.0143e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 762/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5302e-05 - val_loss: 8.9240e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 763/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4874e-05 - val_loss: 8.9003e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 764/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.4662e-05 - val_loss: 8.9855e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 765/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.5140e-05 - val_loss: 8.9647e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 766/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.4967e-05 - val_loss: 8.8928e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 767/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4367e-05 - val_loss: 8.9295e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 768/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4372e-05 - val_loss: 8.9291e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 769/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4927e-05 - val_loss: 8.8868e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 770/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4192e-05 - val_loss: 8.8947e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 771/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4884e-05 - val_loss: 8.8775e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 772/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4198e-05 - val_loss: 8.8226e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 773/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4314e-05 - val_loss: 8.9214e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 774/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4555e-05 - val_loss: 8.9308e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 775/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.4168e-05 - val_loss: 8.9622e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 776/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4218e-05 - val_loss: 8.9044e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 777/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4365e-05 - val_loss: 8.9306e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 778/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.4409e-05 - val_loss: 9.0798e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 779/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.4754e-05 - val_loss: 8.8205e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 780/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4150e-05 - val_loss: 8.9875e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 781/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4494e-05 - val_loss: 8.8593e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 782/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4212e-05 - val_loss: 8.8937e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 783/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.4407e-05 - val_loss: 8.8239e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 784/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4160e-05 - val_loss: 8.8571e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 785/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4366e-05 - val_loss: 8.8645e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 786/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4425e-05 - val_loss: 8.9798e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 787/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4842e-05 - val_loss: 8.8632e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 788/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.4892e-05 - val_loss: 9.0332e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 789/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4185e-05 - val_loss: 8.8361e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 790/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4138e-05 - val_loss: 8.9096e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 791/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4569e-05 - val_loss: 8.8657e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 792/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.4604e-05 - val_loss: 8.8983e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 793/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4536e-05 - val_loss: 8.9642e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 794/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4324e-05 - val_loss: 8.8343e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 795/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4763e-05 - val_loss: 8.8119e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 796/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3949e-05 - val_loss: 8.9168e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 797/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4350e-05 - val_loss: 8.9545e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 798/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4307e-05 - val_loss: 8.9093e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 799/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4362e-05 - val_loss: 8.9265e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 800/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4546e-05 - val_loss: 8.9956e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 801/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.4637e-05 - val_loss: 9.0094e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 802/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4592e-05 - val_loss: 8.8771e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 803/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4277e-05 - val_loss: 8.8141e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 804/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3984e-05 - val_loss: 8.8191e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 805/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4113e-05 - val_loss: 8.7875e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 806/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4194e-05 - val_loss: 8.8253e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 807/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3955e-05 - val_loss: 8.8053e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 808/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4022e-05 - val_loss: 8.8619e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 809/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.3969e-05 - val_loss: 8.9731e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 810/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3997e-05 - val_loss: 8.8810e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 811/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4064e-05 - val_loss: 8.8955e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 812/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4096e-05 - val_loss: 8.9264e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 813/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4037e-05 - val_loss: 8.7554e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 814/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4489e-05 - val_loss: 8.8014e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 815/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3654e-05 - val_loss: 8.8077e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 816/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3786e-05 - val_loss: 8.8831e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 817/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3758e-05 - val_loss: 8.8837e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 818/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.3875e-05 - val_loss: 8.8818e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 819/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3937e-05 - val_loss: 8.8260e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 820/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3658e-05 - val_loss: 8.7897e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 821/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3929e-05 - val_loss: 8.8231e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 822/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.3682e-05 - val_loss: 8.9551e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 823/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3946e-05 - val_loss: 8.8486e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 824/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3627e-05 - val_loss: 8.8044e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 825/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3470e-05 - val_loss: 8.7799e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 826/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3465e-05 - val_loss: 8.8080e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 827/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4163e-05 - val_loss: 8.7656e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 828/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3863e-05 - val_loss: 8.7808e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 829/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4201e-05 - val_loss: 8.8032e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 830/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3211e-05 - val_loss: 8.8601e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 831/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3450e-05 - val_loss: 8.8540e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 832/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4038e-05 - val_loss: 8.7857e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 833/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3480e-05 - val_loss: 8.8305e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 834/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3513e-05 - val_loss: 8.8244e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 835/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3789e-05 - val_loss: 8.7771e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 836/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3338e-05 - val_loss: 8.8532e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 837/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3442e-05 - val_loss: 9.0476e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 838/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3692e-05 - val_loss: 8.8698e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 839/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3170e-05 - val_loss: 8.8151e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 840/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3892e-05 - val_loss: 8.8359e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 841/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3504e-05 - val_loss: 8.7978e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 842/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3388e-05 - val_loss: 8.8148e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 843/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3920e-05 - val_loss: 8.7904e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 844/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.3538e-05 - val_loss: 8.8602e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 845/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3761e-05 - val_loss: 8.7685e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 846/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3416e-05 - val_loss: 8.8713e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 847/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3915e-05 - val_loss: 8.7480e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 848/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3344e-05 - val_loss: 8.7649e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 849/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3599e-05 - val_loss: 9.0074e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 850/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3987e-05 - val_loss: 8.8298e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 851/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3820e-05 - val_loss: 8.7758e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 852/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3779e-05 - val_loss: 8.8079e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 853/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3247e-05 - val_loss: 8.8612e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 854/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3178e-05 - val_loss: 9.0092e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 855/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4228e-05 - val_loss: 8.7681e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 856/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3454e-05 - val_loss: 8.7931e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 857/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3382e-05 - val_loss: 8.8496e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 858/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3762e-05 - val_loss: 8.7764e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 859/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3883e-05 - val_loss: 8.9154e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 860/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.3335e-05 - val_loss: 8.8111e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 861/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3432e-05 - val_loss: 8.7674e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 862/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4229e-05 - val_loss: 8.7649e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 863/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3108e-05 - val_loss: 8.7431e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 864/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3690e-05 - val_loss: 8.8653e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 865/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4037e-05 - val_loss: 8.7107e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 866/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2729e-05 - val_loss: 8.7578e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 867/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3274e-05 - val_loss: 8.7420e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 868/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3458e-05 - val_loss: 8.7174e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 869/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3388e-05 - val_loss: 8.7890e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 870/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4.3705e-05 - val_loss: 8.7000e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 871/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 4.3224e-05 - val_loss: 8.9914e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 872/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3987e-05 - val_loss: 8.7083e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 873/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4.3841e-05 - val_loss: 8.7952e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 874/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3109e-05 - val_loss: 8.8680e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 875/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3848e-05 - val_loss: 8.9101e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 876/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2981e-05 - val_loss: 8.7573e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 877/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2946e-05 - val_loss: 8.7715e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 878/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4.3723e-05 - val_loss: 8.7421e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 879/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2822e-05 - val_loss: 8.8450e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 880/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3346e-05 - val_loss: 8.9155e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 881/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3304e-05 - val_loss: 8.9037e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 882/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3271e-05 - val_loss: 8.7202e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 883/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2605e-05 - val_loss: 8.8811e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 884/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.3903e-05 - val_loss: 8.6898e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 885/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2676e-05 - val_loss: 8.7178e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 886/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2797e-05 - val_loss: 8.7337e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 887/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2831e-05 - val_loss: 8.7199e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 888/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2899e-05 - val_loss: 8.8255e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 889/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3021e-05 - val_loss: 8.7543e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 890/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2684e-05 - val_loss: 8.8141e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 891/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.2706e-05 - val_loss: 8.7634e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 892/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2741e-05 - val_loss: 8.7990e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 893/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3140e-05 - val_loss: 8.7829e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 894/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3006e-05 - val_loss: 8.7291e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 895/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.3055e-05 - val_loss: 8.7370e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 896/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2954e-05 - val_loss: 8.7362e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 897/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2858e-05 - val_loss: 8.7646e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 898/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2968e-05 - val_loss: 8.7073e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 899/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3084e-05 - val_loss: 8.7894e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 900/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3193e-05 - val_loss: 8.7381e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 901/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2863e-05 - val_loss: 8.7242e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 902/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2634e-05 - val_loss: 8.7578e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 903/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2863e-05 - val_loss: 8.7059e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 904/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2803e-05 - val_loss: 8.7969e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 905/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2922e-05 - val_loss: 8.7553e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 906/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3035e-05 - val_loss: 8.7054e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 907/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2883e-05 - val_loss: 8.7269e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 908/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2526e-05 - val_loss: 8.7538e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 909/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3222e-05 - val_loss: 8.7975e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 910/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3526e-05 - val_loss: 8.6766e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 911/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2477e-05 - val_loss: 8.7216e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 912/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2717e-05 - val_loss: 8.6437e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 913/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2701e-05 - val_loss: 8.8253e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 914/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.3655e-05 - val_loss: 8.6911e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 915/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2396e-05 - val_loss: 8.6451e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 916/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2962e-05 - val_loss: 8.6660e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 917/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2733e-05 - val_loss: 8.7704e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 918/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.3015e-05 - val_loss: 8.8262e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 919/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2916e-05 - val_loss: 8.6912e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 920/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2711e-05 - val_loss: 8.7062e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 921/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2358e-05 - val_loss: 8.6943e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 922/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3055e-05 - val_loss: 8.7276e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 923/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.2895e-05 - val_loss: 8.7154e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 924/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2804e-05 - val_loss: 8.6933e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 925/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2456e-05 - val_loss: 8.7810e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 926/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2661e-05 - val_loss: 8.6424e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 927/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2489e-05 - val_loss: 8.6675e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 928/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2653e-05 - val_loss: 8.6415e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 929/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2096e-05 - val_loss: 8.9602e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 930/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3237e-05 - val_loss: 8.8693e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 931/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2623e-05 - val_loss: 8.7131e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 932/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.2423e-05 - val_loss: 8.7429e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 933/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2539e-05 - val_loss: 8.7012e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 934/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2386e-05 - val_loss: 8.6020e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 935/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2336e-05 - val_loss: 8.6380e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 936/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1981e-05 - val_loss: 8.6870e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 937/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3041e-05 - val_loss: 8.7146e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 938/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2648e-05 - val_loss: 8.6491e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 939/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2346e-05 - val_loss: 8.6815e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 940/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2006e-05 - val_loss: 8.7334e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 941/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2701e-05 - val_loss: 8.5905e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 942/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2005e-05 - val_loss: 8.9182e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 943/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3005e-05 - val_loss: 8.6216e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 944/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2241e-05 - val_loss: 8.6463e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 945/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2168e-05 - val_loss: 8.6847e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 946/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2368e-05 - val_loss: 8.6255e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 947/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2366e-05 - val_loss: 8.6453e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 948/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2318e-05 - val_loss: 8.7473e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 949/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2540e-05 - val_loss: 8.6484e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 950/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2372e-05 - val_loss: 8.7748e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 951/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2024e-05 - val_loss: 8.6898e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 952/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2100e-05 - val_loss: 8.7707e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 953/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2562e-05 - val_loss: 8.7278e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 954/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2126e-05 - val_loss: 8.6886e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 955/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.2369e-05 - val_loss: 8.6731e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 956/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2542e-05 - val_loss: 8.6627e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 957/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2625e-05 - val_loss: 8.6351e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 958/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2012e-05 - val_loss: 8.6352e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 959/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2221e-05 - val_loss: 8.6465e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 960/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.2367e-05 - val_loss: 8.6247e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 961/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2028e-05 - val_loss: 8.6192e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 962/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2169e-05 - val_loss: 8.7904e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 963/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2823e-05 - val_loss: 8.6269e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 964/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2159e-05 - val_loss: 8.7111e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 965/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2116e-05 - val_loss: 8.7754e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 966/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.2030e-05 - val_loss: 8.6354e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 967/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.2461e-05 - val_loss: 8.6082e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 968/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1989e-05 - val_loss: 8.7088e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 969/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2130e-05 - val_loss: 8.6038e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 970/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.2187e-05 - val_loss: 8.6885e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 971/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2028e-05 - val_loss: 8.5799e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 972/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2172e-05 - val_loss: 8.7303e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 973/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2127e-05 - val_loss: 8.7011e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 974/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2265e-05 - val_loss: 8.5947e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 975/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1889e-05 - val_loss: 8.6718e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 976/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.2125e-05 - val_loss: 8.5969e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 977/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2018e-05 - val_loss: 8.5634e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 978/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2369e-05 - val_loss: 8.6351e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 979/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2130e-05 - val_loss: 8.6030e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 980/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.2116e-05 - val_loss: 8.5925e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 981/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2213e-05 - val_loss: 8.5713e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 982/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1975e-05 - val_loss: 8.6232e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 983/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2247e-05 - val_loss: 8.8035e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 984/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1921e-05 - val_loss: 8.6146e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 985/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1744e-05 - val_loss: 8.6606e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 986/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2358e-05 - val_loss: 8.5693e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 987/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1722e-05 - val_loss: 8.5822e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 988/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1687e-05 - val_loss: 8.6255e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 989/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1983e-05 - val_loss: 8.6061e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 990/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1600e-05 - val_loss: 8.5648e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 991/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1702e-05 - val_loss: 8.5849e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 992/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1701e-05 - val_loss: 8.5537e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 993/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1529e-05 - val_loss: 8.5921e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 994/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1745e-05 - val_loss: 8.6691e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 995/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1431e-05 - val_loss: 8.5901e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 996/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1664e-05 - val_loss: 8.6469e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 997/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2006e-05 - val_loss: 8.6587e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 998/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1580e-05 - val_loss: 8.5501e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 999/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1620e-05 - val_loss: 8.5817e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1000/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1625e-05 - val_loss: 8.5829e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1001/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1866e-05 - val_loss: 8.6273e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1002/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2047e-05 - val_loss: 8.6238e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1003/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1739e-05 - val_loss: 8.7647e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1004/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2156e-05 - val_loss: 8.9090e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1005/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2000e-05 - val_loss: 8.6832e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1006/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1663e-05 - val_loss: 8.6591e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1007/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2117e-05 - val_loss: 8.6244e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1008/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1703e-05 - val_loss: 8.5762e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1009/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2039e-05 - val_loss: 8.5755e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1010/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1950e-05 - val_loss: 8.5641e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1011/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1617e-05 - val_loss: 8.7750e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1012/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2043e-05 - val_loss: 8.5162e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1013/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1845e-05 - val_loss: 8.6007e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1014/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1943e-05 - val_loss: 8.5420e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1015/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1357e-05 - val_loss: 8.5908e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1016/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2211e-05 - val_loss: 8.6096e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1017/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1740e-05 - val_loss: 8.5452e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1018/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1200e-05 - val_loss: 8.6817e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1019/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2007e-05 - val_loss: 8.5477e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1020/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1617e-05 - val_loss: 8.5363e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1021/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1227e-05 - val_loss: 8.5591e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1022/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1896e-05 - val_loss: 8.5993e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1023/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1604e-05 - val_loss: 8.5504e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1024/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1666e-05 - val_loss: 8.5808e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1025/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1623e-05 - val_loss: 8.5797e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1026/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1190e-05 - val_loss: 8.6993e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1027/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1609e-05 - val_loss: 8.5447e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1028/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1967e-05 - val_loss: 8.5196e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1029/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1289e-05 - val_loss: 8.6129e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1030/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1669e-05 - val_loss: 8.6364e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1031/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1520e-05 - val_loss: 8.6572e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1032/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1331e-05 - val_loss: 8.5944e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1033/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0969e-05 - val_loss: 8.5569e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1034/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1181e-05 - val_loss: 8.6517e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1035/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1249e-05 - val_loss: 8.6374e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1036/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1558e-05 - val_loss: 8.5602e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1037/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1707e-05 - val_loss: 8.5254e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1038/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1245e-05 - val_loss: 8.8187e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1039/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1703e-05 - val_loss: 8.4915e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1040/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1522e-05 - val_loss: 8.6540e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1041/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 4.1374e-05 - val_loss: 8.6747e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1042/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1380e-05 - val_loss: 8.5622e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1043/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0837e-05 - val_loss: 8.5502e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1044/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0940e-05 - val_loss: 8.5204e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1045/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1280e-05 - val_loss: 8.6971e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1046/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1722e-05 - val_loss: 8.7074e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1047/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1879e-05 - val_loss: 8.5194e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1048/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0972e-05 - val_loss: 8.5746e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1049/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1247e-05 - val_loss: 8.5562e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1050/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1328e-05 - val_loss: 8.5613e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1051/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1453e-05 - val_loss: 8.6602e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1052/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1598e-05 - val_loss: 8.5613e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1053/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1063e-05 - val_loss: 8.5609e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1054/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1444e-05 - val_loss: 8.5764e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1055/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1347e-05 - val_loss: 8.5910e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1056/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1198e-05 - val_loss: 8.6610e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1057/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1276e-05 - val_loss: 8.5474e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1058/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0651e-05 - val_loss: 8.5262e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1059/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0707e-05 - val_loss: 8.5343e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1060/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0978e-05 - val_loss: 8.5424e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1061/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1564e-05 - val_loss: 8.8062e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1062/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1357e-05 - val_loss: 8.5239e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1063/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0742e-05 - val_loss: 8.5396e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1064/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1222e-05 - val_loss: 8.5176e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1065/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1313e-05 - val_loss: 8.6032e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1066/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1418e-05 - val_loss: 8.5044e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1067/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1212e-05 - val_loss: 8.5020e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1068/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0814e-05 - val_loss: 8.5689e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1069/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0842e-05 - val_loss: 8.5327e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1070/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0868e-05 - val_loss: 8.5449e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1071/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0893e-05 - val_loss: 8.5267e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1072/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1156e-05 - val_loss: 8.5417e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1073/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1165e-05 - val_loss: 8.6252e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1074/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1314e-05 - val_loss: 8.5282e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1075/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1278e-05 - val_loss: 8.5728e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1076/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0642e-05 - val_loss: 8.6506e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1077/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0725e-05 - val_loss: 8.5856e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1078/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 4.0955e-05 - val_loss: 8.5129e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1079/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0933e-05 - val_loss: 8.5035e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1080/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0814e-05 - val_loss: 8.7012e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1081/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4.0962e-05 - val_loss: 8.5653e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1082/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1391e-05 - val_loss: 8.5181e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1083/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1020e-05 - val_loss: 8.4393e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1084/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4.0650e-05 - val_loss: 8.5538e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1085/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0748e-05 - val_loss: 8.5375e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1086/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0841e-05 - val_loss: 8.4705e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1087/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0834e-05 - val_loss: 8.5519e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1088/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1080e-05 - val_loss: 8.4700e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1089/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0900e-05 - val_loss: 8.5927e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1090/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1072e-05 - val_loss: 8.4836e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1091/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0974e-05 - val_loss: 8.5059e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1092/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0858e-05 - val_loss: 8.4335e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1093/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0698e-05 - val_loss: 8.4533e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1094/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0342e-05 - val_loss: 8.4909e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1095/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1141e-05 - val_loss: 8.4982e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1096/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0292e-05 - val_loss: 8.6981e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1097/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0872e-05 - val_loss: 8.5887e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1098/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0545e-05 - val_loss: 8.5866e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1099/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0554e-05 - val_loss: 8.4618e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1100/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0271e-05 - val_loss: 8.5821e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1101/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1124e-05 - val_loss: 8.4561e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1102/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0622e-05 - val_loss: 8.4382e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1103/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0875e-05 - val_loss: 8.4581e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1104/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 4.0575e-05 - val_loss: 8.4586e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1105/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0410e-05 - val_loss: 8.4838e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1106/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0416e-05 - val_loss: 8.5077e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1107/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1656e-05 - val_loss: 8.5971e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1108/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0700e-05 - val_loss: 8.4971e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1109/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0851e-05 - val_loss: 8.4784e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1110/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0840e-05 - val_loss: 8.4304e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1111/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0671e-05 - val_loss: 8.5433e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1112/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0261e-05 - val_loss: 8.5526e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1113/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0140e-05 - val_loss: 8.3935e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1114/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0620e-05 - val_loss: 8.5021e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1115/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0610e-05 - val_loss: 8.5201e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1116/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0129e-05 - val_loss: 8.4713e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1117/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0728e-05 - val_loss: 8.4847e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1118/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0714e-05 - val_loss: 8.4366e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1119/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0374e-05 - val_loss: 8.4545e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1120/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0638e-05 - val_loss: 8.4455e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1121/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0323e-05 - val_loss: 8.4447e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1122/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9934e-05 - val_loss: 8.4584e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1123/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9913e-05 - val_loss: 8.5062e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1124/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0754e-05 - val_loss: 8.4749e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1125/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0554e-05 - val_loss: 8.4384e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1126/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0580e-05 - val_loss: 8.5456e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1127/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9997e-05 - val_loss: 8.6130e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1128/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0512e-05 - val_loss: 8.5092e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1129/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0554e-05 - val_loss: 8.4960e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1130/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0171e-05 - val_loss: 8.4825e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1131/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0639e-05 - val_loss: 8.4281e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1132/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0088e-05 - val_loss: 8.4369e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1133/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0210e-05 - val_loss: 8.3971e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1134/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0042e-05 - val_loss: 8.5322e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1135/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0481e-05 - val_loss: 8.4673e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1136/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0325e-05 - val_loss: 8.5012e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1137/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0223e-05 - val_loss: 8.4453e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1138/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0024e-05 - val_loss: 8.5373e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1139/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0547e-05 - val_loss: 8.5729e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1140/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9976e-05 - val_loss: 8.4760e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1141/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0132e-05 - val_loss: 8.5617e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1142/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0225e-05 - val_loss: 8.3953e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1143/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0480e-05 - val_loss: 8.4840e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1144/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0680e-05 - val_loss: 8.3977e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1145/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0033e-05 - val_loss: 8.4455e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1146/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9980e-05 - val_loss: 8.5039e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1147/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0285e-05 - val_loss: 8.4967e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1148/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0321e-05 - val_loss: 8.4401e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1149/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0519e-05 - val_loss: 8.4978e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1150/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0239e-05 - val_loss: 8.3973e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1151/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0233e-05 - val_loss: 8.4449e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1152/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0263e-05 - val_loss: 8.4408e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1153/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9840e-05 - val_loss: 8.4274e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1154/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0119e-05 - val_loss: 8.4478e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1155/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0304e-05 - val_loss: 8.4035e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1156/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9899e-05 - val_loss: 8.4188e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1157/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0134e-05 - val_loss: 8.4389e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1158/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0152e-05 - val_loss: 8.5600e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1159/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4.0674e-05 - val_loss: 8.4179e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1160/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4.0141e-05 - val_loss: 8.4757e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1161/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0089e-05 - val_loss: 8.3867e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1162/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9537e-05 - val_loss: 8.5981e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1163/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0420e-05 - val_loss: 8.4444e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1164/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.9906e-05 - val_loss: 8.5227e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1165/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0575e-05 - val_loss: 8.5037e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1166/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0319e-05 - val_loss: 8.3893e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1167/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9650e-05 - val_loss: 8.4225e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1168/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9872e-05 - val_loss: 8.4458e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1169/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9611e-05 - val_loss: 8.3966e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1170/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9923e-05 - val_loss: 8.4608e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1171/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0260e-05 - val_loss: 8.4664e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1172/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.9908e-05 - val_loss: 8.5400e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1173/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9902e-05 - val_loss: 8.4206e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1174/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0241e-05 - val_loss: 8.3849e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1175/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0163e-05 - val_loss: 8.4687e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1176/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9943e-05 - val_loss: 8.3974e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1177/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9861e-05 - val_loss: 8.4153e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1178/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0147e-05 - val_loss: 8.3978e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1179/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.9874e-05 - val_loss: 8.6376e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1180/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0700e-05 - val_loss: 8.4703e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1181/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9887e-05 - val_loss: 8.3775e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1182/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9418e-05 - val_loss: 8.4286e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1183/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0004e-05 - val_loss: 8.5633e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1184/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0066e-05 - val_loss: 8.3833e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1185/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9493e-05 - val_loss: 8.3806e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1186/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9657e-05 - val_loss: 8.4554e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1187/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9506e-05 - val_loss: 8.3976e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1188/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.9834e-05 - val_loss: 8.4754e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1189/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9945e-05 - val_loss: 8.5840e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1190/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9627e-05 - val_loss: 8.4754e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1191/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9924e-05 - val_loss: 8.4254e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1192/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0110e-05 - val_loss: 8.3665e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1193/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9951e-05 - val_loss: 8.3660e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1194/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9542e-05 - val_loss: 8.3619e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1195/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.9626e-05 - val_loss: 8.4381e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1196/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9866e-05 - val_loss: 8.4444e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1197/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9884e-05 - val_loss: 8.5158e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1198/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9931e-05 - val_loss: 8.4161e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1199/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.9433e-05 - val_loss: 8.5077e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1200/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9700e-05 - val_loss: 8.4963e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1201/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9638e-05 - val_loss: 8.2991e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1202/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.9622e-05 - val_loss: 8.3634e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1203/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9886e-05 - val_loss: 8.4768e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1204/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9692e-05 - val_loss: 8.4259e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1205/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9568e-05 - val_loss: 8.3546e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1206/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9377e-05 - val_loss: 8.3227e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1207/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9851e-05 - val_loss: 8.4041e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1208/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9669e-05 - val_loss: 8.3903e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1209/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9655e-05 - val_loss: 8.4094e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1210/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.9594e-05 - val_loss: 8.5072e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1211/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.9492e-05 - val_loss: 8.3092e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1212/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9748e-05 - val_loss: 8.4424e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1213/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9327e-05 - val_loss: 8.3616e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1214/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9383e-05 - val_loss: 8.3950e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1215/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9648e-05 - val_loss: 8.4218e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1216/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0057e-05 - val_loss: 8.3587e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1217/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9363e-05 - val_loss: 8.3947e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1218/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.9712e-05 - val_loss: 8.3689e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1219/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9533e-05 - val_loss: 8.3389e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1220/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9523e-05 - val_loss: 8.4233e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1221/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9114e-05 - val_loss: 8.3078e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1222/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9257e-05 - val_loss: 8.3796e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1223/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9848e-05 - val_loss: 8.4011e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1224/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9600e-05 - val_loss: 8.4193e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1225/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.9583e-05 - val_loss: 8.3412e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1226/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9212e-05 - val_loss: 8.3096e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1227/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9266e-05 - val_loss: 8.3297e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1228/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9579e-05 - val_loss: 8.3962e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1229/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9714e-05 - val_loss: 8.6734e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1230/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9173e-05 - val_loss: 8.3672e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1231/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9359e-05 - val_loss: 8.4700e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1232/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9545e-05 - val_loss: 8.3015e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1233/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9632e-05 - val_loss: 8.3287e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1234/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.9388e-05 - val_loss: 8.3533e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1235/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9122e-05 - val_loss: 8.3541e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1236/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9099e-05 - val_loss: 8.4181e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1237/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8987e-05 - val_loss: 8.3326e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1238/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9829e-05 - val_loss: 8.5313e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1239/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9464e-05 - val_loss: 8.5337e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1240/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9847e-05 - val_loss: 8.3686e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1241/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9269e-05 - val_loss: 8.3919e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1242/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9037e-05 - val_loss: 8.3413e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1243/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9222e-05 - val_loss: 8.3723e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1244/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9026e-05 - val_loss: 8.4980e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1245/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9594e-05 - val_loss: 8.3893e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1246/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9326e-05 - val_loss: 8.3883e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1247/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9092e-05 - val_loss: 8.3613e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1248/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9420e-05 - val_loss: 8.3093e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1249/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9296e-05 - val_loss: 8.2919e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1250/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8992e-05 - val_loss: 8.3845e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1251/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9462e-05 - val_loss: 8.5088e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1252/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.9356e-05 - val_loss: 8.3665e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1253/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9201e-05 - val_loss: 8.3569e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1254/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9316e-05 - val_loss: 8.3748e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1255/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9478e-05 - val_loss: 8.3664e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1256/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9432e-05 - val_loss: 8.3271e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1257/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8912e-05 - val_loss: 8.4089e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1258/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9445e-05 - val_loss: 8.3324e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1259/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9116e-05 - val_loss: 8.3462e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1260/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9080e-05 - val_loss: 8.3053e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1261/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9207e-05 - val_loss: 8.3648e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1262/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.8953e-05 - val_loss: 8.4046e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1263/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8592e-05 - val_loss: 8.3263e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1264/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 3.8848e-05 - val_loss: 8.4109e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1265/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9358e-05 - val_loss: 8.3883e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1266/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9113e-05 - val_loss: 8.3940e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1267/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9210e-05 - val_loss: 8.3652e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1268/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9504e-05 - val_loss: 8.3691e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1269/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9404e-05 - val_loss: 8.3107e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1270/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 3.8791e-05 - val_loss: 8.3399e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1271/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9233e-05 - val_loss: 8.3936e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1272/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8784e-05 - val_loss: 8.5429e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1273/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8960e-05 - val_loss: 8.2960e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1274/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.9087e-05 - val_loss: 8.3601e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1275/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3.9174e-05 - val_loss: 8.3746e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1276/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9481e-05 - val_loss: 8.3809e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1277/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9057e-05 - val_loss: 8.4542e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1278/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9298e-05 - val_loss: 8.3481e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1279/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8950e-05 - val_loss: 8.4177e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1280/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8993e-05 - val_loss: 8.4029e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1281/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.8966e-05 - val_loss: 8.3455e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1282/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8714e-05 - val_loss: 8.4211e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1283/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8859e-05 - val_loss: 8.3075e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1284/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8673e-05 - val_loss: 8.3330e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1285/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8800e-05 - val_loss: 8.3294e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1286/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9018e-05 - val_loss: 8.3572e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1287/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.8781e-05 - val_loss: 8.4232e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1288/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9219e-05 - val_loss: 8.2521e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1289/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8751e-05 - val_loss: 8.2884e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1290/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9021e-05 - val_loss: 8.3158e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1291/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8906e-05 - val_loss: 8.3155e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1292/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9135e-05 - val_loss: 8.2748e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1293/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8999e-05 - val_loss: 8.3264e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1294/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8611e-05 - val_loss: 8.4574e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1295/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.8652e-05 - val_loss: 8.2312e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1296/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.8965e-05 - val_loss: 8.2930e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1297/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8463e-05 - val_loss: 8.3445e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1298/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9436e-05 - val_loss: 8.2858e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1299/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.8722e-05 - val_loss: 8.2553e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1300/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8590e-05 - val_loss: 8.3642e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1301/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8956e-05 - val_loss: 8.2984e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1302/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3.8760e-05 - val_loss: 8.3168e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1303/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8817e-05 - val_loss: 8.3046e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1304/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.8649e-05 - val_loss: 8.2860e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1305/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.8821e-05 - val_loss: 8.3180e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1306/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8463e-05 - val_loss: 8.2591e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1307/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8516e-05 - val_loss: 8.3093e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1308/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8406e-05 - val_loss: 8.2875e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1309/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8513e-05 - val_loss: 8.2957e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1310/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8939e-05 - val_loss: 8.3960e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1311/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8551e-05 - val_loss: 8.3134e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1312/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.8521e-05 - val_loss: 8.3509e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1313/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.8715e-05 - val_loss: 8.3144e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1314/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8431e-05 - val_loss: 8.3240e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1315/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.8493e-05 - val_loss: 8.3533e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1316/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9277e-05 - val_loss: 8.4333e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1317/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8831e-05 - val_loss: 8.2882e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1318/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8384e-05 - val_loss: 8.2386e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1319/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8574e-05 - val_loss: 8.3059e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1320/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.8698e-05 - val_loss: 8.3113e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1321/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8270e-05 - val_loss: 8.3849e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1322/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8853e-05 - val_loss: 8.3140e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1323/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8819e-05 - val_loss: 8.3057e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1324/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.8433e-05 - val_loss: 8.3165e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1325/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.8484e-05 - val_loss: 8.2519e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1326/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7974e-05 - val_loss: 8.2264e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1327/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8652e-05 - val_loss: 8.2561e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1328/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8337e-05 - val_loss: 8.2829e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1329/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8794e-05 - val_loss: 8.3092e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1330/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8594e-05 - val_loss: 8.3307e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1331/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.8405e-05 - val_loss: 8.2633e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1332/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8458e-05 - val_loss: 8.2563e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1333/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8465e-05 - val_loss: 8.3294e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1334/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.8514e-05 - val_loss: 8.3239e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1335/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8436e-05 - val_loss: 8.4280e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1336/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9024e-05 - val_loss: 8.2561e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1337/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8985e-05 - val_loss: 8.3603e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1338/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.8522e-05 - val_loss: 8.2857e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1339/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.8332e-05 - val_loss: 8.3289e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1340/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8821e-05 - val_loss: 8.2677e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1341/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8714e-05 - val_loss: 8.2989e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1342/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9070e-05 - val_loss: 8.2464e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1343/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.8622e-05 - val_loss: 8.3527e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1344/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.8461e-05 - val_loss: 8.3304e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1345/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8356e-05 - val_loss: 8.2759e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1346/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.8064e-05 - val_loss: 8.3283e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1347/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8872e-05 - val_loss: 8.2626e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1348/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.8555e-05 - val_loss: 8.1978e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1349/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8679e-05 - val_loss: 8.2582e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1350/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8077e-05 - val_loss: 8.2675e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1351/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8193e-05 - val_loss: 8.2886e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1352/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.8347e-05 - val_loss: 8.2710e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1353/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.8301e-05 - val_loss: 8.2276e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1354/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8670e-05 - val_loss: 8.3233e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1355/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 3.8291e-05 - val_loss: 8.3594e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1356/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.8280e-05 - val_loss: 8.3699e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1357/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8359e-05 - val_loss: 8.2672e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1358/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.8454e-05 - val_loss: 8.2510e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1359/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8268e-05 - val_loss: 8.2319e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1360/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.8225e-05 - val_loss: 8.2206e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1361/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8235e-05 - val_loss: 8.6534e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1362/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8585e-05 - val_loss: 8.2414e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1363/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8179e-05 - val_loss: 8.2315e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1364/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.8305e-05 - val_loss: 8.2585e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1365/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8425e-05 - val_loss: 8.4060e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1366/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8719e-05 - val_loss: 8.2687e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1367/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7952e-05 - val_loss: 8.1870e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1368/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8097e-05 - val_loss: 8.2703e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1369/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8258e-05 - val_loss: 8.2180e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1370/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8453e-05 - val_loss: 8.2007e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1371/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7617e-05 - val_loss: 8.2339e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1372/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8239e-05 - val_loss: 8.1853e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1373/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8115e-05 - val_loss: 8.2837e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1374/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.8378e-05 - val_loss: 8.2718e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1375/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8094e-05 - val_loss: 8.2792e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1376/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8199e-05 - val_loss: 8.1654e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1377/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8175e-05 - val_loss: 8.2247e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1378/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7666e-05 - val_loss: 8.2737e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1379/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8275e-05 - val_loss: 8.1788e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1380/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.8084e-05 - val_loss: 8.1927e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1381/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7811e-05 - val_loss: 8.2364e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1382/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8212e-05 - val_loss: 8.2374e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1383/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8391e-05 - val_loss: 8.2872e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1384/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8385e-05 - val_loss: 8.2176e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1385/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7793e-05 - val_loss: 8.2620e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1386/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7784e-05 - val_loss: 8.1744e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1387/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8071e-05 - val_loss: 8.2128e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1388/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8296e-05 - val_loss: 8.4564e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1389/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8495e-05 - val_loss: 8.3384e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1390/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.8022e-05 - val_loss: 8.3179e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1391/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8424e-05 - val_loss: 8.2204e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1392/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7915e-05 - val_loss: 8.2876e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1393/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8086e-05 - val_loss: 8.2040e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1394/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7974e-05 - val_loss: 8.1728e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1395/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7920e-05 - val_loss: 8.2311e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1396/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7887e-05 - val_loss: 8.1962e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1397/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8017e-05 - val_loss: 8.3230e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1398/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8802e-05 - val_loss: 8.2532e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1399/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.8307e-05 - val_loss: 8.2520e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1400/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.8220e-05 - val_loss: 8.2228e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1401/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8197e-05 - val_loss: 8.1752e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1402/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8274e-05 - val_loss: 8.2700e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1403/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7780e-05 - val_loss: 8.3558e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1404/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8182e-05 - val_loss: 8.2304e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1405/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7656e-05 - val_loss: 8.2084e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1406/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7763e-05 - val_loss: 8.2574e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1407/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7806e-05 - val_loss: 8.2110e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1408/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7946e-05 - val_loss: 8.1672e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1409/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.7614e-05 - val_loss: 8.1674e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1410/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7606e-05 - val_loss: 8.1945e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1411/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7944e-05 - val_loss: 8.1635e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1412/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.7720e-05 - val_loss: 8.1821e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1413/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7876e-05 - val_loss: 8.2498e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1414/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7418e-05 - val_loss: 8.2170e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1415/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.8646e-05 - val_loss: 8.2595e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1416/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7902e-05 - val_loss: 8.2216e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1417/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7976e-05 - val_loss: 8.2579e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1418/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7772e-05 - val_loss: 8.2131e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1419/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7865e-05 - val_loss: 8.1365e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1420/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7496e-05 - val_loss: 8.1540e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1421/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7678e-05 - val_loss: 8.1960e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1422/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7867e-05 - val_loss: 8.3287e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1423/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7805e-05 - val_loss: 8.1440e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1424/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7752e-05 - val_loss: 8.1982e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1425/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7792e-05 - val_loss: 8.1758e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1426/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7861e-05 - val_loss: 8.1860e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1427/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7681e-05 - val_loss: 8.2681e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1428/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7415e-05 - val_loss: 8.2275e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1429/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7811e-05 - val_loss: 8.2185e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1430/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.7831e-05 - val_loss: 8.2234e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1431/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7432e-05 - val_loss: 8.2489e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1432/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.7457e-05 - val_loss: 8.1863e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1433/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7859e-05 - val_loss: 8.2138e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1434/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7737e-05 - val_loss: 8.3157e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1435/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7677e-05 - val_loss: 8.3984e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1436/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7850e-05 - val_loss: 8.2010e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1437/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7713e-05 - val_loss: 8.1948e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1438/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7394e-05 - val_loss: 8.2281e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1439/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7918e-05 - val_loss: 8.2148e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1440/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7500e-05 - val_loss: 8.1240e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1441/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.7771e-05 - val_loss: 8.1578e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1442/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7509e-05 - val_loss: 8.1828e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1443/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7460e-05 - val_loss: 8.2344e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1444/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7466e-05 - val_loss: 8.2008e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1445/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7511e-05 - val_loss: 8.1602e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1446/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7684e-05 - val_loss: 8.2299e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1447/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7817e-05 - val_loss: 8.1970e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1448/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7514e-05 - val_loss: 8.2494e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1449/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7897e-05 - val_loss: 8.2160e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1450/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7644e-05 - val_loss: 8.1952e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1451/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7271e-05 - val_loss: 8.2272e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1452/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.7134e-05 - val_loss: 8.2473e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1453/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7579e-05 - val_loss: 8.1327e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1454/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7474e-05 - val_loss: 8.1429e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1455/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7312e-05 - val_loss: 8.1660e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1456/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7139e-05 - val_loss: 8.1654e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1457/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7822e-05 - val_loss: 8.2608e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1458/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 3.7595e-05 - val_loss: 8.2676e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1459/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 3.7692e-05 - val_loss: 8.2753e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1460/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 3.7612e-05 - val_loss: 8.0999e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1461/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7431e-05 - val_loss: 8.1159e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1462/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7165e-05 - val_loss: 8.2719e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1463/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.7498e-05 - val_loss: 8.3183e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1464/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7487e-05 - val_loss: 8.1118e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1465/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7417e-05 - val_loss: 8.1468e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1466/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7484e-05 - val_loss: 8.2142e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1467/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.7698e-05 - val_loss: 8.1362e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1468/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7183e-05 - val_loss: 8.2633e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1469/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7330e-05 - val_loss: 8.3077e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1470/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7507e-05 - val_loss: 8.1318e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1471/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7231e-05 - val_loss: 8.1411e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1472/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.6983e-05 - val_loss: 8.1441e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1473/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7329e-05 - val_loss: 8.2256e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1474/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7138e-05 - val_loss: 8.1727e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1475/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7777e-05 - val_loss: 8.1441e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1476/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7475e-05 - val_loss: 8.1584e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1477/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7443e-05 - val_loss: 8.3032e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1478/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.6990e-05 - val_loss: 8.1489e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1479/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7442e-05 - val_loss: 8.2441e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1480/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7258e-05 - val_loss: 8.2521e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1481/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7639e-05 - val_loss: 8.2282e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1482/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7689e-05 - val_loss: 8.0987e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1483/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7408e-05 - val_loss: 8.1582e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1484/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7524e-05 - val_loss: 8.1482e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1485/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7545e-05 - val_loss: 8.3972e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1486/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.7249e-05 - val_loss: 8.2194e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1487/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7197e-05 - val_loss: 8.1158e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1488/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7250e-05 - val_loss: 8.2689e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1489/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7201e-05 - val_loss: 8.1334e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1490/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7029e-05 - val_loss: 8.1898e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1491/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7196e-05 - val_loss: 8.1175e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1492/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.6926e-05 - val_loss: 8.1483e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1493/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7154e-05 - val_loss: 8.2641e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1494/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7207e-05 - val_loss: 8.1424e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1495/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7272e-05 - val_loss: 8.1497e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1496/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7113e-05 - val_loss: 8.2420e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1497/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7585e-05 - val_loss: 8.1596e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1498/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7140e-05 - val_loss: 8.1314e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1499/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7067e-05 - val_loss: 8.1471e-05 - learning_rate: 5.0000e-05\n",
            "Epoch 1500/1500\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.7148e-05 - val_loss: 8.1325e-05 - learning_rate: 5.0000e-05\n"
          ]
        }
      ],
      "source": [
        "Training = True\n",
        "\n",
        "\n",
        "# Define the learning rate scheduler function\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch < 20:\n",
        "        return lr\n",
        "    else:\n",
        "        return max(lr * 0.99, 5e-5)\n",
        "\n",
        "# Initialize the neural network model\n",
        "model_hf = Sequential()\n",
        "\n",
        "# Add layers to the model\n",
        "model_hf.add(Dense(128, input_shape=(n_f,), activation='gelu'))\n",
        "model_hf.add(Dense(128, activation='gelu'))\n",
        "model_hf.add(Dense(128, activation='sigmoid'))\n",
        "model_hf.add(Dense(128, activation='sigmoid'))\n",
        "#model_hf.add(Dense(128, activation='sigmoid'))\n",
        "model_hf.add(Dense(25, activation='exponential'))\n",
        "\n",
        "\n",
        "if Training:\n",
        "    # Compile the model\n",
        "    initial_learning_rate = 0.001\n",
        "    optimizer = Adam(learning_rate=initial_learning_rate)\n",
        "    model_hf.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "\n",
        "    # Define the learning rate scheduler callback\n",
        "    lr_scheduler = LearningRateScheduler(scheduler)\n",
        "\n",
        "    # Train the model\n",
        "    history_hf = model_hf.fit(X_train2, y_train_f, \n",
        "                    epochs=1500, \n",
        "                    batch_size=64, \n",
        "                    validation_data=(X_test2, y_test_f),\n",
        "                    callbacks=[lr_scheduler])\n",
        "    \n",
        "    model_hf.save('./models2/model_HF_10000_4.keras')\n",
        "\n",
        "model_hf = load_model('./models2/model_HF_10000_4.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394us/step - loss: 6.5688e-05\n",
            "Test accuracy: 6.492153625003994e-05\n",
            "Test rmse: 0.008057390163696923\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss = model_hf.evaluate(x=X_test2, y=y_test_f)\n",
        "\n",
        "# Print the test accuracy\n",
        "print(f\"Test accuracy: {test_loss}\")\n",
        "print(f\"Test rmse: {np.sqrt(test_loss)}\")\n",
        "\n",
        "if Training:\n",
        "    plt.plot(history_hf.history['loss'], label='Loss')\n",
        "    plt.plot(history_hf.history['val_loss'], label='Val loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Learning Rate')\n",
        "    plt.title('Learning Rate over Epochs')\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5 ) Evaluation of different models "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### LOW FIDELITY MODELS "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MODEL LF 1 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_lf = Sequential()\n",
        "\n",
        "# Add layers to the model\n",
        "model_lf.add(Dense(64, input_shape=(n_c,), activation='gelu'))\n",
        "model_lf.add(Dense(64, activation='gelu'))\n",
        "model_lf.add(Dense(64, activation='gelu'))\n",
        "model_lf.add(Dense(32, activation='gelu'))\n",
        "model_lf.add(Dense(25, activation='exponential'))\n",
        "\n",
        "model_lf = load_model('./models2/model_LF_20000_1.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269us/step - loss: 8.8789e-05\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339us/step - loss: 1.0536e-04\n",
            "Training accuracy: 8.865338895702735e-05\n",
            "Test accuracy: 0.00010331796511309221\n",
            "Test rmse: 0.010164544510852034\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test set\n",
        "training_loss = model_lf.evaluate(x=X_train, y=y_train)\n",
        "test_loss = model_lf.evaluate(x=X_test, y=y_test)\n",
        "\n",
        "# Print the test accuracy\n",
        "print(f\"Training accuracy: {training_loss}\")\n",
        "print(f\"Test accuracy: {test_loss}\")\n",
        "print(f\"Test rmse: {np.sqrt(test_loss)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MODEL LF 2 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_lf = Sequential()\n",
        "\n",
        "# Add layers to the model\n",
        "model_lf.add(Dense(128, input_shape=(n_c,), activation='gelu'))\n",
        "model_lf.add(Dense(32, activation='gelu'))\n",
        "model_lf.add(Dense(25, activation='exponential'))\n",
        "\n",
        "model_lf = load_model('./models2/model_LF_20000_2.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277us/step - loss: 7.6902e-05\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 765us/step - loss: 9.6502e-05\n",
            "Training accuracy: 7.636886584805325e-05\n",
            "Test accuracy: 9.426909673493356e-05\n",
            "Test rmse: 0.009709227401546096\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test set\n",
        "training_loss = model_lf.evaluate(x=X_train, y=y_train)\n",
        "test_loss = model_lf.evaluate(x=X_test, y=y_test)\n",
        "\n",
        "# Print the test accuracy\n",
        "print(f\"Training accuracy: {training_loss}\")\n",
        "print(f\"Test accuracy: {test_loss}\")\n",
        "print(f\"Test rmse: {np.sqrt(test_loss)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MODEL LF 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_lf = Sequential()\n",
        "\n",
        "# Add layers to the model\n",
        "model_lf.add(Dense(64, input_shape=(n_c,), activation='gelu'))\n",
        "model_lf.add(Dense(64, activation='gelu'))\n",
        "model_lf.add(Dense(64, activation='sigmoid'))\n",
        "model_lf.add(Dense(32, activation='sigmoid'))\n",
        "model_lf.add(Dense(25, activation='exponential'))\n",
        "\n",
        "model_lf = load_model('./models2/model_LF_20000_3.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266us/step - loss: 1.0813e-04\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355us/step - loss: 1.2543e-04\n",
            "Training accuracy: 0.000106691790279001\n",
            "Test accuracy: 0.00012393121141940355\n",
            "Test rmse: 0.011132439598731428\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test set\n",
        "training_loss = model_lf.evaluate(x=X_train, y=y_train)\n",
        "test_loss = model_lf.evaluate(x=X_test, y=y_test)\n",
        "\n",
        "# Print the test accuracy\n",
        "print(f\"Training accuracy: {training_loss}\")\n",
        "print(f\"Test accuracy: {test_loss}\")\n",
        "print(f\"Test rmse: {np.sqrt(test_loss)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MODEL LF 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_lf = Sequential()\n",
        "\n",
        "# Add layers to the model\n",
        "model_lf.add(Dense(128, input_shape=(n_c,), activation='sigmoid'))\n",
        "model_lf.add(Dense(32, activation='sigmoid'))\n",
        "model_lf.add(Dense(25, activation='exponential'))\n",
        "\n",
        "model_lf = load_model('./models2/model_LF_20000_4.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251us/step - loss: 1.1529e-04\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 332us/step - loss: 1.3978e-04\n",
            "Training accuracy: 0.00011442756658652797\n",
            "Test accuracy: 0.00013789217337034643\n",
            "Test rmse: 0.011742749821500347\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test set\n",
        "training_loss = model_lf.evaluate(x=X_train, y=y_train)\n",
        "test_loss = model_lf.evaluate(x=X_test, y=y_test)\n",
        "\n",
        "# Print the test accuracy\n",
        "print(f\"Training accuracy: {training_loss}\")\n",
        "print(f\"Test accuracy: {test_loss}\")\n",
        "print(f\"Test rmse: {np.sqrt(test_loss)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MODEL LF 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_lf = Sequential()\n",
        "\n",
        "# Add layers to the model\n",
        "model_lf.add(Dense(64, input_shape=(n_c,), activation='tanh'))\n",
        "model_lf.add(Dense(64, activation='tanh'))\n",
        "model_lf.add(Dense(64, activation='tanh'))\n",
        "model_lf.add(Dense(32, activation='tanh'))\n",
        "model_lf.add(Dense(25, activation='exponential'))\n",
        "\n",
        "model_lf = load_model('./models2/model_LF_20000_5.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253us/step - loss: 1.0666e-04\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308us/step - loss: 1.2530e-04\n",
            "Training accuracy: 0.00010532289888942614\n",
            "Test accuracy: 0.00012537431030068547\n",
            "Test rmse: 0.01119706704010856\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test set\n",
        "training_loss = model_lf.evaluate(x=X_train, y=y_train)\n",
        "test_loss = model_lf.evaluate(x=X_test, y=y_test)\n",
        "\n",
        "# Print the test accuracy\n",
        "print(f\"Training accuracy: {training_loss}\")\n",
        "print(f\"Test accuracy: {test_loss}\")\n",
        "print(f\"Test rmse: {np.sqrt(test_loss)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MODEL LF 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_lf = Sequential()\n",
        "\n",
        "# Add layers to the model\n",
        "model_lf.add(Dense(128, input_shape=(n_c,), activation='tanh'))\n",
        "model_lf.add(Dense(32, activation='tanh'))\n",
        "model_lf.add(Dense(25, activation='exponential'))\n",
        "\n",
        "model_lf = load_model('./models2/model_LF_20000_6.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250us/step - loss: 8.1384e-05\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 306us/step - loss: 9.8105e-05\n",
            "Training accuracy: 8.083967259153724e-05\n",
            "Test accuracy: 9.838914411375299e-05\n",
            "Test rmse: 0.009919130209537174\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test set\n",
        "training_loss = model_lf.evaluate(x=X_train, y=y_train)\n",
        "test_loss = model_lf.evaluate(x=X_test, y=y_test)\n",
        "\n",
        "# Print the test accuracy\n",
        "print(f\"Training accuracy: {training_loss}\")\n",
        "print(f\"Test accuracy: {test_loss}\")\n",
        "print(f\"Test rmse: {np.sqrt(test_loss)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### HIGH-FIDELITY MODELS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "HF MODEL 1 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_hf = Sequential()\n",
        "\n",
        "model_hf.add(Dense(256, input_shape=(n_f,), activation='gelu'))\n",
        "model_hf.add(Dense(128, activation='gelu'))\n",
        "model_hf.add(Dense(64, activation='gelu'))\n",
        "model_hf.add(Dense(25, activation='exponential'))\n",
        "\n",
        "model_hf = load_model('./models2/model_HF_10000_1.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387us/step - loss: 1.5807e-05\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 366us/step - loss: 6.6935e-05\n",
            "Training accuracy: 1.570426502439659e-05\n",
            "Test accuracy: 6.657352059846744e-05\n",
            "Test rmse: 0.008159259807020943\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test set\n",
        "training_loss = model_hf.evaluate(x=X_train2, y=y_train_f)\n",
        "test_loss = model_hf.evaluate(x=X_test2, y=y_test_f)\n",
        "\n",
        "# Print the test accuracy\n",
        "print(f\"Training accuracy: {training_loss}\")\n",
        "print(f\"Test accuracy: {test_loss}\")\n",
        "print(f\"Test rmse: {np.sqrt(test_loss)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "HF MODEL 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_hf = Sequential()\n",
        "\n",
        "model_hf.add(Dense(128, input_shape=(n_f,), activation='gelu'))\n",
        "model_hf.add(Dense(128, activation='gelu'))\n",
        "model_hf.add(Dense(128, activation='gelu'))\n",
        "model_hf.add(Dense(128, activation='gelu'))\n",
        "model_hf.add(Dense(25, activation='exponential'))\n",
        "\n",
        "model_hf = load_model('./models2/model_HF_10000_2.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step - loss: 2.6294e-05\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321us/step - loss: 7.2768e-05\n",
            "Training accuracy: 2.6203328161500394e-05\n",
            "Test accuracy: 7.10481617716141e-05\n",
            "Test rmse: 0.008429007164050468\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test set\n",
        "training_loss = model_hf.evaluate(x=X_train2, y=y_train_f)\n",
        "test_loss = model_hf.evaluate(x=X_test2, y=y_test_f)\n",
        "\n",
        "# Print the test accuracy\n",
        "print(f\"Training accuracy: {training_loss}\")\n",
        "print(f\"Test accuracy: {test_loss}\")\n",
        "print(f\"Test rmse: {np.sqrt(test_loss)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MODEL HF 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_hf = Sequential()\n",
        "\n",
        "model_hf.add(Dense(256, input_shape=(n_f,), activation='sigmoid'))\n",
        "model_hf.add(Dense(128, activation='sigmoid'))\n",
        "model_hf.add(Dense(64, activation='sigmoid'))\n",
        "model_hf.add(Dense(25, activation='exponential'))\n",
        "\n",
        "model_hf = load_model('./models2/model_HF_10000_3.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 341us/step - loss: 4.5821e-05\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326us/step - loss: 1.1261e-04\n",
            "Training accuracy: 4.605853246175684e-05\n",
            "Test accuracy: 0.00011159319547004998\n",
            "Test rmse: 0.01056376805264343\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test set\n",
        "training_loss = model_hf.evaluate(x=X_train2, y=y_train_f)\n",
        "test_loss = model_hf.evaluate(x=X_test2, y=y_test_f)\n",
        "\n",
        "# Print the test accuracy\n",
        "print(f\"Training accuracy: {training_loss}\")\n",
        "print(f\"Test accuracy: {test_loss}\")\n",
        "print(f\"Test rmse: {np.sqrt(test_loss)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MODEL HF 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_hf = Sequential()\n",
        "\n",
        "model_hf.add(Dense(128, input_shape=(n_f,), activation='gelu'))\n",
        "model_hf.add(Dense(128, activation='gelu'))\n",
        "model_hf.add(Dense(128, activation='sigmoid'))\n",
        "model_hf.add(Dense(128, activation='sigmoid'))\n",
        "model_hf.add(Dense(25, activation='exponential'))\n",
        "\n",
        "model_hf = load_model('./models2/model_HF_10000_4.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355us/step - loss: 3.6509e-05\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335us/step - loss: 8.3082e-05\n",
            "Training accuracy: 3.6445340811042115e-05\n",
            "Test accuracy: 8.132493530865759e-05\n",
            "Test rmse: 0.009018033893740784\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test set\n",
        "training_loss = model_hf.evaluate(x=X_train2, y=y_train_f)\n",
        "test_loss = model_hf.evaluate(x=X_test2, y=y_test_f)\n",
        "\n",
        "# Print the test accuracy\n",
        "print(f\"Training accuracy: {training_loss}\")\n",
        "print(f\"Test accuracy: {test_loss}\")\n",
        "print(f\"Test rmse: {np.sqrt(test_loss)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MODEL HF 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_hf = Sequential()\n",
        "\n",
        "model_hf.add(Dense(256, input_shape=(n_f,), activation='tanh'))\n",
        "model_hf.add(Dense(128, activation='tanh'))\n",
        "model_hf.add(Dense(64, activation='tanh'))\n",
        "model_hf.add(Dense(25, activation='exponential'))\n",
        "\n",
        "model_hf = load_model('./models2/model_HF_10000_5.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335us/step - loss: 3.0644e-05\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387us/step - loss: 9.5726e-05\n",
            "Training accuracy: 3.0602419428760186e-05\n",
            "Test accuracy: 9.235160541720688e-05\n",
            "Test rmse: 0.009609974267249985\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test set\n",
        "training_loss = model_hf.evaluate(x=X_train2, y=y_train_f)\n",
        "test_loss = model_hf.evaluate(x=X_test2, y=y_test_f)\n",
        "\n",
        "# Print the test accuracy\n",
        "print(f\"Training accuracy: {training_loss}\")\n",
        "print(f\"Test accuracy: {test_loss}\")\n",
        "print(f\"Test rmse: {np.sqrt(test_loss)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MODEL HF 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/lucacaroselli/miniconda3/envs/myenv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "model_hf = Sequential()\n",
        "\n",
        "model_hf.add(Dense(128, input_shape=(n_f,), activation='tanh'))\n",
        "model_hf.add(Dense(128, activation='tanh'))\n",
        "model_hf.add(Dense(128, activation='tanh'))\n",
        "model_hf.add(Dense(128, activation='tanh'))\n",
        "model_hf.add(Dense(25, activation='exponential'))\n",
        "\n",
        "model_hf = load_model('./models2/model_HF_10000_6.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324us/step - loss: 4.2619e-05\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 311us/step - loss: 9.2118e-05\n",
            "Training accuracy: 4.304098547436297e-05\n",
            "Test accuracy: 9.004707681015134e-05\n",
            "Test rmse: 0.009489313821881503\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test set\n",
        "training_loss = model_hf.evaluate(x=X_train2, y=y_train_f)\n",
        "test_loss = model_hf.evaluate(x=X_test2, y=y_test_f)\n",
        "\n",
        "# Print the test accuracy\n",
        "print(f\"Training accuracy: {training_loss}\")\n",
        "print(f\"Test accuracy: {test_loss}\")\n",
        "print(f\"Test rmse: {np.sqrt(test_loss)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BONUS "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This model would be too big to be used as a low fidelity model yet was calculataed to set a benchmark for the performane of larger Networks. \n",
        "\n",
        "To be more specific this model will help highlight the contribution that the knowledge on the POD coefficient give to the same Neural Network (model_POD_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "\n",
        "# Add layers to the model\n",
        "model.add(Dense(256, input_shape=(n_c,), activation='gelu'))\n",
        "model.add(Dense(256, activation='gelu'))\n",
        "model.add(Dense(256, activation='gelu'))\n",
        "model.add(Dense(25, activation='exponential'))\n",
        "\n",
        "model = load_model('./models/model_LF_64000_4.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 732us/step - loss: 1.3567e-05\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 908us/step - loss: 1.3916e-05\n",
            "Training accuracy: 1.3542909982788842e-05\n",
            "Test accuracy: 1.3684657460544258e-05\n",
            "Test rmse: 0.0036992779647580225\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test set\n",
        "training_loss = model.evaluate(x=X_train, y=y_train)\n",
        "test_loss = model.evaluate(x=X_test, y=y_test)\n",
        "\n",
        "# Print the test accuracy\n",
        "print(f\"Training accuracy: {training_loss}\")\n",
        "print(f\"Test accuracy: {test_loss}\")\n",
        "print(f\"Test rmse: {np.sqrt(test_loss)}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOLKvMRTaykHpCaydJMPm9E",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
